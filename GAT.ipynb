{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e98fb86-748e-487f-ad93-7c5ae315a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics as sk\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import networkx as nx\n",
    "from torch_geometric.typing import SparseTensor\n",
    "from GraphRicciCurvature.FormanRicci import FormanRicci\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchviz import make_dot\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898a86e2-5d23-48ea-91ca-6def5db33357",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGES_PATH = './data/images/train'\n",
    "TEST_IMAGES_PATH = './data/images/test'\n",
    "VAL_IMAGES_PATH = './data/images/val'\n",
    "\n",
    "TRAIN_NPZ_FILE = './data/images/npz/train_images.npz'\n",
    "TEST_NPZ_FILE = './data/images/npz/test_images.npz'\n",
    "VAL_NPZ_FILE = './data/images/npz/val_images.npz'\n",
    "\n",
    "NUM_FEATURES = 224 * 224 * 3\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_type = \"GAT\"\n",
    "max_hop = 1\n",
    "layers = 3\n",
    "hidden_channels = 2048\n",
    "dropout = 0.5\n",
    "batch_norm = True\n",
    "lr = 0.0005\n",
    "num_batch = 3\n",
    "num_epoch = 400\n",
    "multilabel = False\n",
    "do_evaluation = True\n",
    "residual = True\n",
    "print_result = True\n",
    "aggregations_flow = 100\n",
    "max_communities = 1000\n",
    "remove_edges = 0\n",
    "make_unbalanced = True\n",
    "dense = True\n",
    "topological_measure = \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ef908-8888-45f3-8ccc-9087f3198620",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963478c4-d010-4f23-bcba-d9ad86e71429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz_as_tensors(file_path):\n",
    "    \"\"\"\n",
    "    Load the .npz files as tensors\n",
    "\n",
    "    Args:\n",
    "        file_path (string): To get the .npz file and load it as a tensor\n",
    "    \"\"\"\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "    images = data['images']\n",
    "    labels = data['labels']\n",
    "\n",
    "    images_tensor = torch.tensor(images, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return images_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd73e6-723e-4e50-b41f-51a40596f7a9",
   "metadata": {},
   "source": [
    "## Process Data and Save Masked Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f19001-e79a-4f13-a8b2-180e7621bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(sparsity):\n",
    "    \"\"\"\n",
    "    Process and save the data in both sparse and dense formats.\n",
    "\n",
    "    Args:\n",
    "        sparsity (bool): Whether to process the dataset as sparse or dense\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load training data\n",
    "    train_images, train_labels = load_npz_as_tensors(TRAIN_NPZ_FILE)\n",
    "    train_data = train_images.reshape(train_images.shape[0], NUM_FEATURES)\n",
    "    num_train = train_images.shape[0]\n",
    "\n",
    "    # Load validation data\n",
    "    val_images, val_labels = load_npz_as_tensors(VAL_NPZ_FILE)\n",
    "    val_data = val_images.reshape(val_images.shape[0], NUM_FEATURES)\n",
    "    num_val = val_images.shape[0]\n",
    "\n",
    "    # Load test data\n",
    "    test_images, test_labels = load_npz_as_tensors(TEST_NPZ_FILE)\n",
    "    test_data = test_images.reshape(test_images.shape[0], NUM_FEATURES)\n",
    "    num_test = test_images.shape[0]\n",
    "\n",
    "    # Concatenate train, validation, and test data\n",
    "    num_data = num_train + num_val + num_test\n",
    "    data_feat = np.concatenate((train_data, val_data, test_data), axis=0)\n",
    "    data_label = np.concatenate((train_labels, val_labels, test_labels), axis=0).reshape(-1)\n",
    "\n",
    "    # Construct and scale adjacency matrix\n",
    "    adj_matrix = sk.pairwise.cosine_similarity(data_feat, data_feat)\n",
    "    adj_matrix = (adj_matrix - adj_matrix.min())/(adj_matrix.max()-adj_matrix.min())\n",
    "\n",
    "    # Apply sparsity thresholds\n",
    "    threshold = 0.977 if sparsity else 0.970\n",
    "\n",
    "    adj_matrix = adj_matrix > threshold\n",
    "\n",
    "    # Generate masks\n",
    "    train_mask = np.zeros(num_data, dtype=bool)\n",
    "    train_mask[:num_train] = True\n",
    "    val_mask = np.zeros(num_data, dtype=bool)\n",
    "    val_mask[num_train:num_train + num_val] = True\n",
    "    test_mask = np.zeros(num_data, dtype=bool)\n",
    "    test_mask[num_train + num_val:] = True\n",
    "\n",
    "    # Save masks, features, labels, and edge index\n",
    "    suffix = 'sparse' if sparsity else 'dense'\n",
    "    base_path = f\"./data/npy/{suffix}\"\n",
    "\n",
    "    np.save(f\"{base_path}/train_mask.npy\", train_mask)\n",
    "    np.save(f\"{base_path}/val_mask.npy\", val_mask)\n",
    "    np.save(f\"{base_path}/test_mask.npy\", test_mask)\n",
    "    np.save(f\"{base_path}/data_feat.npy\", data_feat)\n",
    "    np.save(f\"{base_path}/data_label.npy\", data_label)\n",
    "\n",
    "    # Generate and save edge index\n",
    "    edge_index = np.array([[i, j] for i in range(num_data) for j in range(num_data) if i != j and adj_matrix[i, j]])\n",
    "    np.save(f\"{base_path}/edge_index.npy\", edge_index)\n",
    "\n",
    "    print(f\"View-({'Sparse' if sparsity else 'Dense'}) generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b9194-f73d-4ec6-8391-3ab695d8c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with sparsity parameter\n",
    "#process_data(sparsity=True)\n",
    "#process_data(sparsity=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a9db9-0c40-4643-9dd8-efd158dd652b",
   "metadata": {},
   "source": [
    "## Load Data, Masks, and Print Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac277c1-033f-4b77-b556-8f3ddc3b7fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics(features, labels, edge_index, train_mask, val_mask, test_mask):\n",
    "    \"\"\"\n",
    "    Print statistics of the dataset\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Node features\n",
    "        labels (np.ndarray): Node labels\n",
    "        edge_index (np.ndarray): Edge indices\n",
    "        train_mask (torch.Tensor): Mask for training nodes\n",
    "        val_mask (torch.Tensor): Mask for validation nodes\n",
    "        test_mask (torch.Tensor): Mask for test nodes\n",
    "    \"\"\"\n",
    "    print(\"=============== Dataset Types ==============\")\n",
    "    print(f\"Type of features: {type(features)}\")\n",
    "    print(f\"Type of labels: {type(labels)}\")\n",
    "    print(f\"Type of edge_index: {type(edge_index)}\")\n",
    "    print(f\"Type of train_mask: {type(train_mask)}\")\n",
    "    print(f\"Type of val_mask: {type(val_mask)}\")\n",
    "    print(f\"Type of test_mask: {type(test_mask)}\")\n",
    "    print(\"=============== Dataset Properties ==================\")\n",
    "    print(f\"Total Nodes: {features.shape[0]}\")\n",
    "    print(f\"Total Edges: {edge_index.shape[0]}\")\n",
    "    print(f\"Number of Features: {features.shape[1]}\")\n",
    "    if labels.ndim == 1:\n",
    "        print(f\"Number of Labels: {labels.max() + 1}\")\n",
    "        print(\"Task Type: Multi-class Classification\")\n",
    "    else:\n",
    "        print(f\"Number of Labels: {labels.shape[1]}\")\n",
    "        print(\"Task Type: Multi-label Classification\")\n",
    "    print(f\"Training Nodes: {train_mask.sum().item()}\")\n",
    "    print(f\"Validation Nodes: {val_mask.sum().item()}\")\n",
    "    print(f\"Testing Nodes: {test_mask.sum().item()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27431e-7abd-45e7-a7a0-4549b3bde472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(sparse=True, balanced=True):\n",
    "    \"\"\"\n",
    "    Load the dataset in either sparse or dense format\n",
    "\n",
    "    Args:\n",
    "        sparse (bool): Whether to load the sparse or dense version of the dataset\n",
    "    \"\"\"\n",
    "    print(f\"Loading Dataset\")\n",
    "    \n",
    "    # Load masks\n",
    "    suffix = 'sparse' if sparse else 'dense'\n",
    "    \n",
    "    train_mask = torch.tensor(np.load(f\"./data/npy/{suffix}/train_mask.npy\"))\n",
    "    val_mask = torch.tensor(np.load(f\"./data/npy/{suffix}/val_mask.npy\"))\n",
    "    test_mask = torch.tensor(np.load(f\"./data/npy/{suffix}/test_mask.npy\"))\n",
    "\n",
    "    # Load labels\n",
    "    labels = np.load(f\"./data/npy/{suffix}/data_label.npy\")\n",
    "\n",
    "    # Load and normalize features\n",
    "    features = np.load(f\"./data/npy/{suffix}/data_feat.npy\")\n",
    "    features = sklearn.preprocessing.StandardScaler().fit_transform(features)\n",
    "\n",
    "    # Load edge indices\n",
    "    edge_index = np.load(f\"./data/npy/{suffix}/edge_index.npy\")\n",
    "\n",
    "    # Print dataset statistics\n",
    "    print_statistics(features, labels, edge_index, train_mask, val_mask, test_mask)\n",
    "\n",
    "    if not balanced:\n",
    "        all_labels = [0, 1, 2, 3, 4, 5]\n",
    "        chosen_labels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "        print(\"[Before unbalancing] Class distribution in the training set:\")\n",
    "        for label in all_labels:\n",
    "            count = np.sum(labels[train_mask] == label)\n",
    "            print(f\"Label {label}: {count} samples\")\n",
    "        \n",
    "        print(\"[Before unbalancing] Class distribution in the validation set:\")\n",
    "        for label in all_labels:\n",
    "            count = np.sum(labels[val_mask] == label)\n",
    "            print(f\"Label {label}: {count} samples\")\n",
    "\n",
    "        print(\"[Before unbalancing] Class distribution in the test set:\")\n",
    "        for label in all_labels:\n",
    "            count = np.sum(labels[test_mask] == label)\n",
    "            print(f\"Label {label}: {count} samples\")\n",
    "\n",
    "        chosen_indices = np.where(np.isin(labels[train_mask], chosen_labels))[0]\n",
    "        train_indices, test_indices = train_test_split(chosen_indices, test_size=0.8, stratify=labels[train_mask][chosen_indices])\n",
    "                \n",
    "        new_train_mask = torch.full_like(train_mask, False)\n",
    "        new_train_mask[train_indices] = True\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            if label in chosen_labels and new_train_mask[i] == False:\n",
    "                    train_mask[i] = False\n",
    "\n",
    "        train_mask[train_indices] = True\n",
    "        test_mask[test_indices] = True\n",
    "\n",
    "        print(\"Class distribution in the training set:\")\n",
    "        for label in all_labels:\n",
    "            count = np.sum(labels[train_mask] == label)\n",
    "            print(f\"Label {label}: {count} samples\")\n",
    "\n",
    "        print(\"Class distribution in the validation set:\")\n",
    "        for label in all_labels:\n",
    "            count = np.sum(labels[val_mask] == label)\n",
    "            print(f\"Label {label}: {count} samples\")\n",
    "\n",
    "        print(\"Class distribution in the test set:\")\n",
    "        for label in all_labels:\n",
    "            count = np.sum(labels[test_mask] == label)\n",
    "            print(f\"Label {label}: {count} samples\")\n",
    "            \n",
    "    return features, labels, edge_index, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b726dd58-d4a0-427b-99ee-766ebf2c5c62",
   "metadata": {},
   "source": [
    "## Graphs and Graph Constructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b907c-4c74-425d-9737-3c168fbb78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(x, y, edge_index, train_mask, val_mask, test_mask):\n",
    "    \"\"\"\n",
    "    Construct a NetworkX graph from node features, labels, and edge information.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray or torch.Tensor): Node features with shape (num_nodes, feature_dim).\n",
    "        y (np.ndarray or torch.Tensor): Node labels with shape (num_nodes,).\n",
    "        edge_index (torch.Tensor or list of tuples): Edge indices either in PyTorch Geometric format (2xN tensor) or standard edge list format.\n",
    "        train_mask (np.ndarray or list): Boolean mask indicating training nodes.\n",
    "        val_mask (np.ndarray or list): Boolean mask indicating validation nodes.\n",
    "        test_mask (np.ndarray or list): Boolean mask indicating test nodes.\n",
    "\n",
    "    Returns:\n",
    "        nx.Graph: A NetworkX graph with nodes having attributes for features, labels, and masks, and edges with default weights.\n",
    "    \"\"\"\n",
    "    # Construct NetworkX Graph\n",
    "    nodes = [i for i in range(x.shape[0])]\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes with attributes\n",
    "    for i in nodes:\n",
    "        G.add_node(i, x=x[i], y=y[i], train=train_mask[i], val=val_mask[i], test=test_mask[i])\n",
    "    \n",
    "    # Handle edge_index input (PyTorch Geometric format)\n",
    "    if isinstance(edge_index, torch.Tensor) and edge_index.dim() == 2 and edge_index.shape[0] == 2:\n",
    "        edge_list = edge_index.t().tolist()\n",
    "    else:\n",
    "        # Assuming edge_index is in standard edge list format\n",
    "        edge_list = edge_index\n",
    "\n",
    "    # Add edges with a default weight of 1\n",
    "    weighted_edges = [(edge[0], edge[1], 1) for edge in edge_list]\n",
    "    G.add_weighted_edges_from(weighted_edges)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942c1b2-68ca-4a4e-8dc8-85da2d508cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_graph(G, multilabel = True):\n",
    "    \"\"\"\n",
    "    Split the graph into training, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        G (nx.Graph): The input NetworkX graph with node attributes specifying train, val, and test masks.\n",
    "        multilabel (bool): Flag indicating if the graph is multilabel. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - x_train (np.ndarray or torch.Tensor): Node features for training nodes.\n",
    "            - y_train (np.ndarray or torch.Tensor): Node labels for training nodes.\n",
    "            - edge_train (np.ndarray or torch.Tensor): Edge indices for training nodes.\n",
    "            - train_mask (np.ndarray or torch.Tensor): Boolean mask for training nodes.\n",
    "            - x_val (np.ndarray or torch.Tensor): Node features for validation nodes.\n",
    "            - y_val (np.ndarray or torch.Tensor): Node labels for validation nodes.\n",
    "            - edge_val (np.ndarray or torch.Tensor): Edge indices for validation nodes.\n",
    "            - val_mask (np.ndarray or torch.Tensor): Boolean mask for validation nodes.\n",
    "            - x_test (np.ndarray or torch.Tensor): Node features for test nodes.\n",
    "            - y_test (np.ndarray or torch.Tensor): Node labels for test nodes.\n",
    "            - edge_test (np.ndarray or torch.Tensor): Edge indices for test nodes.\n",
    "            - test_mask (np.ndarray or torch.Tensor): Boolean mask for test nodes.\n",
    "    \"\"\"\n",
    "    print(\"Splitting Graph...\")\n",
    "    print(\"=============== Graph Splitting ===============\")\n",
    "    \n",
    "    # Get complete test graph\n",
    "    x_test, y_test, edge_test, _, _, test_mask = convert_graph_to_tensor(G, multilabel=multilabel)\n",
    "    \n",
    "    print(f\"Unlabeled + Test + Validation + Training graph nodes: {x_test.shape[0]}\")\n",
    "    print(f\"Unlabeled + Test + Validation + Training graph edges: {edge_test.shape[0]}\")\n",
    "    print(f\"Total test nodes: {test_mask.sum()}\")\n",
    "    \n",
    "    # Get training + val graph\n",
    "    # remove all test nodes\n",
    "    test_nodes = []\n",
    "    for node in G.nodes(data=True):\n",
    "        if node[1]['test']:\n",
    "            test_nodes.append(node[0])\n",
    "    G.remove_nodes_from(test_nodes)\n",
    "    G = nx.convert_node_labels_to_integers(G, first_label=0, ordering='default')\n",
    "    x_val, y_val, edge_val, _, val_mask, _ = convert_graph_to_tensor(G, multilabel=multilabel)\n",
    "    \n",
    "    print(f\"Unlabeled + Validation + Training graph nodes: {x_val.shape[0]}\")\n",
    "    print(f\"Unlabeled + Validation + Training graph edges: {edge_val.shape[0]}\")\n",
    "    print(f\"Total val nodes: {val_mask.sum()}\")\n",
    "    # Get training graph\n",
    "    # remove all val nodes\n",
    "    val_nodes = []\n",
    "    for node in G.nodes(data=True):\n",
    "        if node[1]['val']:\n",
    "            val_nodes.append(node[0])\n",
    "    G.remove_nodes_from(val_nodes)\n",
    "    G = nx.convert_node_labels_to_integers(G, first_label=0, ordering='default')\n",
    "    \n",
    "    x_train, y_train, edge_train, train_mask, _, _ = convert_graph_to_tensor(G, multilabel = multilabel)\n",
    "    \n",
    "    print(f\"Unlabeled + Training graph nodes: {x_train.shape[0]}\")\n",
    "    print(f\"Unlabeled + Training graph edges: {edge_train.shape[0]}\")\n",
    "    print(f\"Total train nodes: {train_mask.sum()}\")\n",
    "    print()\n",
    "    \n",
    "    return (x_train, y_train, edge_train, train_mask, x_val, y_val, edge_val, \n",
    "            val_mask, x_test, y_test, edge_test, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a1c21-f0bc-46a1-b5c8-6ccd2777c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_graph_to_tensor(G, multilabel = True):\n",
    "    \"\"\"\n",
    "    Convert a NetworkX graph into tensors or numpy arrays for use in machine learning models.\n",
    "\n",
    "    Args:\n",
    "        G (nx.Graph): The input NetworkX graph with node attributes for features, labels, and masks.\n",
    "        multilabel (bool): Flag indicating if the graph is multilabel. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - x (np.ndarray or torch.Tensor): Node features.\n",
    "            - y (np.ndarray or torch.Tensor): Node labels.\n",
    "            - edge_index (np.ndarray or torch.Tensor): Edge indices.\n",
    "            - train_mask (np.ndarray or torch.Tensor): Boolean mask for training nodes.\n",
    "            - val_mask (np.ndarray or torch.Tensor): Boolean mask for validation nodes.\n",
    "            - test_mask (np.ndarray or torch.Tensor): Boolean mask for test nodes.\n",
    "    \"\"\"\n",
    "    x = np.empty((G.number_of_nodes(),G.nodes[0]['x'].shape[0]))\n",
    "    \n",
    "    if multilabel:\n",
    "        y = np.empty((G.number_of_nodes(),G.nodes[0]['y'].shape[0]),dtype = 'int')\n",
    "    else:\n",
    "        y = np.empty((G.number_of_nodes(),),dtype = 'int')\n",
    "        \n",
    "    edge_index = np.array([edge for edge in G.edges()])\n",
    "    train_mask = np.empty((G.number_of_nodes(),),dtype = 'bool')\n",
    "    val_mask = np.empty((G.number_of_nodes(),),dtype = 'bool')\n",
    "    test_mask = np.empty((G.number_of_nodes(),),dtype = 'bool')\n",
    "    \n",
    "    for node in G.nodes(data=True):\n",
    "        x[node[0],:] = node[1]['x']\n",
    "        if multilabel:\n",
    "            y[node[0],:] = node[1]['y']\n",
    "        else:\n",
    "            y[node[0]] = node[1]['y']\n",
    "        \n",
    "        train_mask[node[0]] = node[1]['train']\n",
    "        val_mask[node[0]] = node[1]['val']\n",
    "        test_mask[node[0]] = node[1]['test']\n",
    "    \n",
    "    return x, y, edge_index, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab18f6-8cd8-44db-a653-94a441de7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_normalized_adj(edge_index, num_nodes):\n",
    "    \"\"\"\n",
    "    Construct a normalized adjacency matrix from edge indices.\n",
    "\n",
    "    Args:\n",
    "        edge_index (np.ndarray or torch.Tensor): Edge indices in the format [2, num_edges].\n",
    "        num_nodes (int): Number of nodes in the graph.\n",
    "\n",
    "    Returns:\n",
    "        SparseTensor: Normalized adjacency matrix with self-loops added and GCN normalization applied.\n",
    "    \"\"\"\n",
    "    edge_index = torch.tensor(edge_index)\n",
    "    edge_index = torch.transpose(edge_index,0,1)\n",
    "    edge_index_flip = torch.flip(edge_index,[0]) # re-adds flipped edges that were removed by networkx\n",
    "    edge_index = torch.cat((edge_index, edge_index_flip), 1)\n",
    "    adj = SparseTensor(row=edge_index[0], col=edge_index[1], sparse_sizes=(num_nodes,num_nodes))\n",
    "    adj = adj.set_diag() # adding self loops\n",
    "    adj = gcn_norm(adj, add_self_loops=False) # normalization\n",
    "\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6247bc6-c5b9-45b6-a78a-8badef0c04ac",
   "metadata": {},
   "source": [
    "## Get Metrics of the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733accae-0c75-4a77-86b1-b1d494744d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_to_label(out):\n",
    "    \"\"\"\n",
    "    Convert logits to predicted labels using argmax\n",
    "\n",
    "    Args:\n",
    "        out (torch.Tensor): Logits tensor\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Predicated labels\n",
    "    \"\"\"\n",
    "    return out.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e8356-cd24-4b98-b017-792f501de818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(logits, y):\n",
    "    \"\"\"\n",
    "    Calculate classification metrics.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Model output logits.\n",
    "        y (torch.Tensor): True labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (accuracy, micro F1 score, sensitivity, specificity)\n",
    "    \"\"\"\n",
    "    if y.dim() == 1: # Multi-class\n",
    "        y_pred = logit_to_label(logits)\n",
    "        cm = confusion_matrix(y.cpu(),y_pred.cpu())\n",
    "        FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "        FN = cm.sum(axis=1) - np.diag(cm)\n",
    "        TP = np.diag(cm)\n",
    "        TN = cm.sum() - (FP + FN + TP)\n",
    "    \n",
    "        acc = np.diag(cm).sum() / cm.sum()\n",
    "        micro_f1 = acc # micro f1 = accuracy for multi-class\n",
    "        sens = TP.sum() / (TP.sum() + FN.sum())\n",
    "        spec = TN.sum() / (TN.sum() + FP.sum())\n",
    "    \n",
    "    else: # Multi-label\n",
    "        y_pred = logits >= 0\n",
    "        y_true = y >= 0.5\n",
    "        \n",
    "        tp = int((y_true & y_pred).sum())\n",
    "        tn = int((~y_true & ~y_pred).sum())\n",
    "        fp = int((~y_true & y_pred).sum())\n",
    "        fn = int((y_true & ~y_pred).sum())\n",
    "        \n",
    "        acc = (tp + tn)/(tp + fp + tn + fn)\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        micro_f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        sens = (tp)/(tp + fn)\n",
    "        spec = (tn)/(tn + fp)\n",
    "        \n",
    "    return acc, micro_f1, sens, spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae01a0-14af-4a81-a311-a6da163c6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_metric_compute(metric, graph):\n",
    "    \"\"\"\n",
    "    Compute a specified edge metric by averaging the provided node metrics for each edge.\n",
    "\n",
    "    Args:\n",
    "        metric (dict): A dictionary where keys are nodes and values are the metric values for the nodes.\n",
    "        graph (networkx.Graph): A NetworkX graph object.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are edges and values are the average metric values of the nodes incident to the edge.\n",
    "    \"\"\"\n",
    "    edges = {}\n",
    "    \n",
    "    # Iterate over edges\n",
    "    for edge in graph.edges():\n",
    "        u, v = edge\n",
    "        \n",
    "        metric_u = metric[u]\n",
    "        metric_v = metric[v]\n",
    "        \n",
    "        avg_centrality = (metric_u + metric_v) / 2\n",
    "        \n",
    "        edges[edge] = avg_centrality\n",
    "    \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b85128e-c061-4ed6-8d1a-73d26a7c5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_degree_centrality(graph):\n",
    "    \"\"\"\n",
    "    Compute the degree centrality for each edge in the graph.\n",
    "\n",
    "    Args:\n",
    "        graph (networkx.Graph): A NetworkX graph object.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are edges and values are the average degree of the nodes incident to the edge.\n",
    "    \"\"\"\n",
    "    edge_degree = {}\n",
    "    \n",
    "    # Iterate over edges\n",
    "    for edge in graph.edges():\n",
    "        u, v = edge\n",
    "        \n",
    "        # Compute degrees of the nodes incident to the edge\n",
    "        degree_u = graph.degree(u)\n",
    "        degree_v = graph.degree(v)\n",
    "        \n",
    "        # Calculate average degree\n",
    "        avg_degree = (degree_u + degree_v) / 2\n",
    "        \n",
    "        # Assign average degree as edge degree centrality\n",
    "        edge_degree[edge] = avg_degree\n",
    "    \n",
    "    return edge_degree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcca2e1-3da6-4bf8-8907-7047aa03f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_homophily_ratios(adj, x, y):\n",
    "    \"\"\"\n",
    "    Calculate homophily ratios based on feature and label similarities.\n",
    "\n",
    "    Args:\n",
    "        adj (scipy.sparse.coo_matrix): Adjacency matrix in COO format.\n",
    "        x (numpy.ndarray or torch.Tensor): Node features.\n",
    "        y (numpy.ndarray or torch.Tensor): Node labels.\n",
    "\n",
    "    Returns:\n",
    "        list: List of homophily ratios for each edge in the adjacency matrix.\n",
    "    \"\"\"\n",
    "    homophily_ratios = []\n",
    "    \n",
    "    # Extract COO format components\n",
    "    row, col, _ = adj.coo()\n",
    "    \n",
    "    for r, c in zip(row.tolist(), col.tolist()):\n",
    "        # Extract features and labels using row and col indices\n",
    "        x1, y1 = x[r], y[r]\n",
    "        x2, y2 = x[c], y[c]\n",
    "        \n",
    "        # Calculate similarity in features (assuming features are numpy arrays)\n",
    "        feature_similarity = np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))\n",
    "        \n",
    "        # Calculate similarity in labels\n",
    "        label_similarity = 1 if y1 == y2 else 0\n",
    "        \n",
    "        # Homophily ratio can be a combination of both similarities\n",
    "        homophily_ratio = 0.5 * feature_similarity + 0.5 * label_similarity\n",
    "        \n",
    "        homophily_ratios.append(homophily_ratio)\n",
    "    \n",
    "    return homophily_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2495cc58-6b28-4b07-b074-450e61bdc56e",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb816e6-f97c-45ca-a6fb-eada55656d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_deltas(deltas, num_nodes_to_plot=5):\n",
    "    \"\"\"\n",
    "    Plot the feature variation (delta) over aggregation steps for a specified number of nodes.\n",
    "\n",
    "    Parameters:\n",
    "    - deltas (list of np.array): List where each element is an array of deltas for a specific node.\n",
    "    - num_nodes_to_plot (int): Number of nodes to plot. Defaults to 5.\n",
    "\n",
    "    This function creates a line plot where each line represents the feature variation over aggregation steps for each node.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(num_nodes_to_plot):\n",
    "        plt.plot(deltas[i], label=f'Node {i}')\n",
    "    plt.xlabel('Aggregation Step')\n",
    "    plt.ylabel('Feature Variation (Delta)')\n",
    "    plt.title('Feature Variation Over Aggregation Steps')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d3fd0-622b-40ba-8540-bdb183a55ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mse_results(mse_results, filename='gat_mse_results.png'):\n",
    "    \"\"\"\n",
    "    Plot the Mean Squared Error (MSE) results for each node and save the plot as an image.\n",
    "\n",
    "    Parameters:\n",
    "    - mse_results (list of float): List of MSE values for each node.\n",
    "    - filename (str): Name of the file where the plot will be saved.\n",
    "\n",
    "    This function creates a bar plot where each bar represents the MSE for a specific node and saves the plot to a file.\n",
    "    \"\"\"\n",
    "    num_nodes = len(mse_results)\n",
    "    nodes = np.arange(num_nodes)\n",
    "    filename = \"./results/gnn/images/\" + filename\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(nodes, mse_results, color='skyblue')\n",
    "    plt.xlabel('Nodes')\n",
    "    plt.ylabel('Mean Squared Error (MSE)')\n",
    "    plt.title('Mean Squared Error for Each Node')\n",
    "    plt.xticks(nodes)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a428cb6-142a-4727-aab2-10c7b48790c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_homophily_ratios_for_graph(graph):\n",
    "    \"\"\"\n",
    "    Calculate the homophily ratios for all edges in the graph.\n",
    "\n",
    "    Parameters:\n",
    "    - graph (networkx.Graph): The input graph where each node has 'x' (features) and 'y' (label) attributes.\n",
    "\n",
    "    Returns:\n",
    "    - homophily_ratios (list of float): List of homophily ratios for each edge in the graph.\n",
    "\n",
    "    Homophily ratio is calculated as a combination of feature similarity and label similarity between connected nodes.\n",
    "    \"\"\"\n",
    "    homophily_ratios = []\n",
    "\n",
    "    for edge in graph.edges():\n",
    "        node1, node2 = edge\n",
    "        # Extract features and labels for both nodes\n",
    "        x1, y1 = graph.nodes[node1]['x'], graph.nodes[node1]['y']\n",
    "        x2, y2 = graph.nodes[node2]['x'], graph.nodes[node2]['y']\n",
    "        \n",
    "        # Calculate similarity in features (using cosine similarity)\n",
    "        feature_similarity = np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))\n",
    "        \n",
    "        # Calculate similarity in labels\n",
    "        label_similarity = 1 if y1 == y2 else 0\n",
    "        \n",
    "        # Compute homophily ratio as a weighted combination of feature and label similarity\n",
    "        homophily_ratio = 0.5 * feature_similarity + 0.5 * label_similarity\n",
    "        \n",
    "        homophily_ratios.append(homophily_ratio)\n",
    "    \n",
    "    return homophily_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7fea3a-a100-4f18-93f1-d2f841564e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_homophily_distributions(graphs, stages, dataset):\n",
    "    \"\"\"\n",
    "    Plot the distribution of homophily ratios across different stages.\n",
    "\n",
    "    Parameters:\n",
    "    - graphs (list of networkx.Graph): List of three graphs corresponding to different stages.\n",
    "    - stages (list of str): List of stage names for labeling the plot.\n",
    "    - dataset (str): Name of the dataset for saving the plot file.\n",
    "\n",
    "    This function calculates homophily ratios for each graph and plots their distributions using KDE plots.\n",
    "    The plot is saved to a file with a name based on the dataset.\n",
    "    \"\"\"\n",
    "    if len(graphs) != 3 or len(stages) != 3:\n",
    "        raise ValueError(\"You must provide exactly three graphs and three stages.\")\n",
    "    \n",
    "    homophily_data = [calculate_homophily_ratios(graph) for graph in graphs]\n",
    "    colors = ['#2E8B57', '#FF6347', '#8670FD']  # Different colors for better visual distinction\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    for i in range(3):\n",
    "        sns.kdeplot(homophily_data[i], color=colors[i], fill=True, label=f'{stages[i]}', common_norm=True)\n",
    "\n",
    "    plt.xlabel('Homophily Ratio')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Homophily Ratios Distribution Across Stages')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot to a file\n",
    "    output_file = f'./results/gnn/images/gat_distribution_homophily_{dataset}.png'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf538c-9fa8-46be-87df-5e68aeeceda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degree_distribution(G_old, G_new, output_file='gat_plot_degree_distributions.png'):\n",
    "    \"\"\"\n",
    "    Plot and compare the degree distributions of two graphs.\n",
    "\n",
    "    Parameters:\n",
    "    - G_old (networkx.Graph): The first graph for comparison.\n",
    "    - G_new (networkx.Graph): The second graph for comparison.\n",
    "    - output_file (str): Name of the file where the plot will be saved.\n",
    "\n",
    "    This function plots the degree distributions of the two graphs on the same plot for comparison and saves the plot to a file.\n",
    "    \"\"\"\n",
    "    output_file = './results/gnn/images/' + output_file\n",
    "\n",
    "    # Get degree sequences for both graphs and sort them\n",
    "    degree_sequence_old = sorted((d for n, d in G_old.degree()), reverse=True)\n",
    "    degree_sequence_new = sorted((d for n, d in G_new.degree()), reverse=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the degree distribution for the old graph\n",
    "    plt.plot(degree_sequence_old, 'o', label='G_old', alpha=0.5)\n",
    "    \n",
    "    # Plot the degree distribution for the new graph\n",
    "    plt.plot(degree_sequence_new, 'o', label='G_new', alpha=0.5)\n",
    "\n",
    "    plt.title(\"Degree Distribution Comparison\")\n",
    "    plt.xlabel(\"Rank Node\")\n",
    "    plt.ylabel(\"Degree\")\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0f6b17-08a1-4622-8f14-cb53c4a30605",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc436c79-3a4f-499e-9f20-7a77b58be119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    GAT: Graph Attention Networks\n",
    "    Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio\n",
    "    Graph Attention Networks (ICLR 2018)\n",
    "    https://arxiv.org/abs/1710.10903\n",
    "\"\"\"\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_layers, in_channels, \n",
    "                 out_channels, heads=1, batch_norm=False, \n",
    "                 dropout=0.0, drop_input=False, residual=False, graph_task=False):\n",
    "        \"\"\"\n",
    "        Initialize the Graph Attention Network (GAT) model.\n",
    "\n",
    "        Args:\n",
    "            hidden_channels (int): Number of hidden channels in GAT layers.\n",
    "            num_layers (int): Number of GAT layers in the network.\n",
    "            in_channels (int): Number of input features.\n",
    "            out_channels (int): Number of output features.\n",
    "            heads (int, optional): Number of attention heads. Default is 1.\n",
    "            batch_norm (bool, optional): Whether to apply batch normalization. Default is False.\n",
    "            dropout (float, optional): Dropout rate for dropout layers. Default is 0.0.\n",
    "            drop_input (bool, optional): Whether to apply dropout to input features. Default is False.\n",
    "            residual (bool, optional): Whether to use residual connections. Default is False.\n",
    "            graph_task (bool, optional): Whether the task is a graph-level task. Default is False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "        self.drop_input = drop_input\n",
    "        self.residual = residual\n",
    "        self.graph_task = graph_task\n",
    "        \n",
    "        self.gat_layers = torch.nn.ModuleList()  # List to store GAT layers\n",
    "        self.batch_norm_layers = torch.nn.ModuleList()  # List to store batch normalization layers\n",
    "        \n",
    "        # Adding input layer\n",
    "        self.gat_layers.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm_layers.append(torch.nn.BatchNorm1d(hidden_channels * heads))\n",
    "            \n",
    "        # Adding hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.gat_layers.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n",
    "            if self.batch_norm:\n",
    "                self.batch_norm_layers.append(torch.nn.BatchNorm1d(hidden_channels * heads))\n",
    "        \n",
    "        # Adding output layer\n",
    "        self.gat_layers.append(GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout))\n",
    "        \n",
    "        # Graph-level readout layer if graph_task is True\n",
    "        if graph_task:\n",
    "            self.graph_readout = torch.nn.Linear(out_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the GAT model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input feature matrix.\n",
    "            edge_index (LongTensor): Edge indices in COO format.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output feature matrix or graph-level output based on graph_task.\n",
    "        \"\"\"\n",
    "        # Apply dropout to input features if drop_input is set to True\n",
    "        if self.drop_input:\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            \n",
    "        # Process through hidden GAT layers\n",
    "        for i in range(self.num_layers - 1):  # Exclude the output layer\n",
    "            x = self.gat_layers[i](x, edge_index)\n",
    "            \n",
    "            if self.batch_norm:\n",
    "                x = self.batch_norm_layers[i](x)\n",
    "            \n",
    "            # Activation function and dropout\n",
    "            if self.graph_task:\n",
    "                x = x.tanh()  # Use tanh for graph-level tasks\n",
    "            else:\n",
    "                x = x.relu()  # Use ReLU for node-level tasks\n",
    "                \n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.gat_layers[-1](x, edge_index)\n",
    "        \n",
    "        # Graph-level readout if graph_task is True\n",
    "        if self.graph_task:\n",
    "            x = self.graph_readout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1896d8-065f-4a09-b351-0565d5ecd3bb",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227802d5-4539-4ab6-b4d1-b41ca17cf45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_batch_step(model, optimizer, criterion, x_train, y_train, \n",
    "                    adj_train, train_mask, logging = False, adj_cond = None):\n",
    "    \"\"\"\n",
    "    Perform a single optimization step on the model using a full batch of training data.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to be trained.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for updating model parameters.\n",
    "        criterion (callable): The loss function.\n",
    "        x_train (torch.Tensor): The input features for the training data.\n",
    "        y_train (torch.Tensor): The target labels for the training data.\n",
    "        adj_train (torch.Tensor): The adjacency matrix for the training data.\n",
    "        train_mask (torch.Tensor or None): Mask indicating which nodes to consider for training.\n",
    "        logging (bool, optional): Whether to log training metrics. Defaults to False.\n",
    "        adj_cond (torch.Tensor or None, optional): The condensed adjacency matrix. Defaults to None.\n",
    "    Returns:\n",
    "        loss (torch.Tensor): The computed loss for the current batch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if adj_cond:\n",
    "        out = model(x_train, adj_train, adj_cond)\n",
    "    else:\n",
    "        out = model(x_train, adj_train)\n",
    "    if train_mask == None:\n",
    "        loss = criterion(out, y_train)\n",
    "    else:\n",
    "        loss = criterion(out[train_mask], y_train[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if logging:\n",
    "        acc,micro_f1,sens,spec = metrics(out,y_train)\n",
    "        print(f\"Train accuracy: {acc}, Train micro_f1: {micro_f1},Train Sens: {sens}, Train Spec: {spec}\")\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91389e90-a34a-43c0-9214-ade438e6af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x, y, adj, mask, adj_cond = None):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the provided data.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to be evaluated.\n",
    "        x (torch.Tensor): The input features for the evaluation data.\n",
    "        y (torch.Tensor): The target labels for the evaluation data.\n",
    "        adj (torch.Tensor): The adjacency matrix for the evaluation data.\n",
    "        mask (torch.Tensor): Mask indicating which nodes to consider for evaluation.\n",
    "        adj_cond (torch.Tensor or None, optional): The condensed adjacency matrix. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        acc (float): The accuracy of the model on the evaluation data.\n",
    "        micro_f1 (float): The micro-averaged F1 score on the evaluation data.\n",
    "        sens (float): The sensitivity (recall) of the model on the evaluation data.\n",
    "        spec (float): The specificity of the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        if adj_cond:\n",
    "            out = model(x, adj, adj_cond).squeeze()\n",
    "        else:\n",
    "            out = model(x, adj).squeeze()\n",
    "        acc,micro_f1,sens,spec = metrics(out[mask],y[mask])\n",
    "    \n",
    "    return acc, micro_f1, sens, spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7a68e-c62b-4174-ad7c-033e3d39c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, x_train, y_train, adj_train, adj_train_cond = None, train_mask = None, x_val = None, \n",
    "          y_val = None, adj_val = None, adj_val_cond = None, val_mask = None, x_test = None, \n",
    "          y_test = None, adj_test = None, adj_test_cond = None, test_mask = None, multilabel = True, \n",
    "          lr = 0.0005, num_epoch = 100):\n",
    "    \"\"\"\n",
    "    Train a model using the provided training data and evaluate it on validation and test data.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to be trained.\n",
    "        device (torch.device): The device (CPU or GPU) to run the computations on.\n",
    "        x_train (torch.Tensor): The input features for the training data.\n",
    "        y_train (torch.Tensor): The target labels for the training data.\n",
    "        adj_train (torch.Tensor): The adjacency matrix for the training data.\n",
    "        adj_train_cond (torch.Tensor or None, optional): The condensed adjacency matrix for training. Defaults to None.\n",
    "        train_mask (torch.Tensor or None, optional): Mask indicating which nodes to consider for training. Defaults to None.\n",
    "        x_val (torch.Tensor or None, optional): The input features for the validation data. Defaults to None.\n",
    "        y_val (torch.Tensor or None, optional): The target labels for the validation data. Defaults to None.\n",
    "        adj_val (torch.Tensor or None, optional): The adjacency matrix for the validation data. Defaults to None.\n",
    "        adj_val_cond (torch.Tensor or None, optional): The condensed adjacency matrix for validation. Defaults to None.\n",
    "        val_mask (torch.Tensor or None, optional): Mask indicating which nodes to consider for validation. Defaults to None.\n",
    "        x_test (torch.Tensor or None, optional): The input features for the test data. Defaults to None.\n",
    "        y_test (torch.Tensor or None, optional): The target labels for the test data. Defaults to None.\n",
    "        adj_test (torch.Tensor or None, optional): The adjacency matrix for the test data. Defaults to None.\n",
    "        adj_test_cond (torch.Tensor or None, optional): The condensed adjacency matrix for testing. Defaults to None.\n",
    "        test_mask (torch.Tensor or None, optional): Mask indicating which nodes to consider for testing. Defaults to None.\n",
    "        multilabel (bool, optional): Whether the task is multilabel classification. Defaults to True.\n",
    "        lr (float, optional): Learning rate for the optimizer. Defaults to 0.0005.\n",
    "        num_epoch (int, optional): Number of training epochs. Defaults to 100.\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - max_val_acc (float): Best validation accuracy achieved.\n",
    "            - max_val_f1 (float): Best validation F1 score achieved.\n",
    "            - max_val_sens (float): Best validation sensitivity achieved.\n",
    "            - max_val_spec (float): Best validation specificity achieved.\n",
    "            - max_val_test_acc (float): Best test accuracy achieved.\n",
    "            - max_val_test_f1 (float): Best test F1 score achieved.\n",
    "            - max_val_test_sens (float): Best test sensitivity achieved.\n",
    "            - max_val_test_spec (float): Best test specificity achieved.\n",
    "            - session_memory (float): Peak GPU memory usage during the session (in MB).\n",
    "            - train_memory (float): Peak GPU memory usage during training (in MB).\n",
    "            - train_time_avg (float): Average time per epoch during training.\n",
    "    \"\"\"\n",
    "\n",
    "    # passing model and training data to GPU\n",
    "    model = model.to(device)\n",
    "    x_train = x_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    adj_train = adj_train.to(device)\n",
    "    if adj_train_cond:\n",
    "        adj_train_cond = adj_train_cond.to(device)\n",
    "    if train_mask != None:\n",
    "        train_mask = train_mask.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    if multilabel:\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    max_val_acc = 0\n",
    "    max_val_sens = 0\n",
    "    max_val_spec = 0\n",
    "    max_val_f1 = 0\n",
    "    max_val_test_acc = 0\n",
    "    max_val_test_sens = 0\n",
    "    max_val_test_spec = 0\n",
    "    max_val_test_f1 = 0\n",
    "    \n",
    "    time_arr = np.zeros((num_epoch,))\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "            \n",
    "        # single mini batch step\n",
    "        t = time.time()\n",
    "\n",
    "        loss = full_batch_step(model, optimizer, criterion, \n",
    "                                   x_train, y_train, adj_train, train_mask, \n",
    "                                   logging=False, adj_cond=adj_train_cond)\n",
    "        \n",
    "        time_per_epoch = time.time() - t\n",
    "        time_arr[epoch] = time_per_epoch\n",
    "        \n",
    "        if epoch == 0:\n",
    "            train_memory = torch.cuda.max_memory_allocated(device)*2**(-20)\n",
    "            \n",
    "            # passing validation and test data to GPU (we do it after first forward pass to get)\n",
    "            # accurate pure training GPU memory usage\n",
    "            if x_val != None and y_val != None and adj_val != None and val_mask != None:\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "                adj_val = adj_val.to(device)\n",
    "                val_mask = val_mask.to(device)\n",
    "                if adj_val_cond:\n",
    "                    adj_val_cond = adj_val_cond.to(device)\n",
    "                if x_test != None and y_test != None and adj_test != None and test_mask != None:\n",
    "                    x_test = x_test.to(device)\n",
    "                    y_test = y_test.to(device)\n",
    "                    adj_test = adj_test.to(device)\n",
    "                    test_mask = test_mask.to(device)\n",
    "                    if adj_test_cond:\n",
    "                        adj_test_cond = adj_test_cond.to(device)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {loss:.10f}, training time: {time_per_epoch:.5f}')\n",
    "            print(f\"Peak GPU Memory Usage: {torch.cuda.max_memory_allocated(device)*2**(-20)} MB\")\n",
    "        \n",
    "        # evaluation\n",
    "        if x_val != None and y_val != None:\n",
    "            acc, micro_f1, sens, spec = evaluate(model, x_val, y_val, adj_val, \n",
    "                                                 val_mask, adj_cond = adj_val_cond)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Val accuracy: {acc}, Val micro_f1: {micro_f1}, Val Sens: {sens}, Val Spec: {spec}\")\n",
    "            \n",
    "            if acc > max_val_acc:\n",
    "                max_val_acc = acc\n",
    "                max_val_f1 = micro_f1\n",
    "                max_val_sens = sens\n",
    "                max_val_spec = spec\n",
    "                \n",
    "                if (x_test != None and y_test != None):\n",
    "                    acc, micro_f1, sens, spec = evaluate(model, x_test, y_test, \n",
    "                                                         adj_test, test_mask, adj_cond = adj_test_cond)\n",
    "                    max_val_test_acc = acc\n",
    "                    max_val_test_f1 = micro_f1\n",
    "                    max_val_test_sens = sens\n",
    "                    max_val_test_spec = spec\n",
    "                    \n",
    "                    print(\"===========================================Best Model Update:=======================================\")\n",
    "                    print(f\"Val accuracy: {max_val_acc}, Val f1: {max_val_f1}, Val Sens: {max_val_sens}, Val Spec: {max_val_spec}\")\n",
    "                    print(f\"Test accuracy: {max_val_test_acc}, Test f1: {max_val_test_f1}, Test Sens: {max_val_test_sens}, Test Spec: {max_val_test_spec}\")\n",
    "                    print(\"====================================================================================================\")\n",
    "\n",
    "    print(\"Best Model:\")\n",
    "    print(f\"Val accuracy: {max_val_acc}, Val f1: {max_val_f1}, Val Sens: {max_val_sens}, Val Spec: {max_val_spec}\")\n",
    "    print(f\"Test accuracy: {max_val_test_acc}, Test f1: {max_val_test_f1}, Test Sens: {max_val_test_sens}, Test Spec: {max_val_test_spec}\")\n",
    "    print(f\"Average time per epoch: {time_arr[10:].mean()}\") # don't include the first few epoch (slower due to Torch initialization)\n",
    "    print(f\"Training GPU Memory Usage: {train_memory} MB\")\n",
    "    print(f\"Peak GPU Memory Usage: {torch.cuda.max_memory_allocated(device)*2**(-20)} MB\")\n",
    "    \n",
    "    # cleaning memory and stats\n",
    "    session_memory = torch.cuda.max_memory_allocated(device)*2**(-20)\n",
    "    train_time_avg = time_arr[10:].mean()\n",
    "    del x_val\n",
    "    del y_val\n",
    "    del x_test\n",
    "    del y_test\n",
    "    model = model.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    \n",
    "    return (max_val_acc, max_val_f1, max_val_sens, max_val_spec, max_val_test_acc,\n",
    "            max_val_test_f1, max_val_test_sens, max_val_test_spec, session_memory, \n",
    "            train_memory, train_time_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e58450-ce6a-41b6-8e72-596d14dd97cd",
   "metadata": {},
   "source": [
    "## Calculate Mean and Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1479bce2-a4e3-4b76-a44b-89e986a51c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_and_variance(filename):\n",
    "    \"\"\"\n",
    "    Calculates mean and variance of specified columns grouped by unique running settings.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the CSV file containing the data.\n",
    "    \"\"\"\n",
    "    # Read the data from a CSV file\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    # Define the columns that represent the unique running setting\n",
    "    grouping_columns = ['Dataset', 'Model_Type', 'Node_Num', 'Hidden_Dimension', 'Max_Communities', 'Remove_Edges', 'Epochs','Topological_Measure','Make_Unbalanced','Dense']\n",
    "\n",
    "    # Define the columns for which we want to calculate the mean and variance\n",
    "    measurement_columns = ['Session_Memory', 'Train_Memory', 'Train_Time_Avg', 'Max_Val_Acc', 'Max_Val_F1', \n",
    "                           'Max_Val_Sens', 'Max_Val_Spec', 'Max_Val_Test_Acc', 'Max_Val_Test_F1', \n",
    "                           'Max_Val_Test_Sens', 'Max_Val_Test_Spec']\n",
    "\n",
    "    # Group the data by the unique running setting\n",
    "    grouped_data = data.groupby(grouping_columns)\n",
    "\n",
    "    # Calculate the mean and variance for each group\n",
    "    mean_data = grouped_data[measurement_columns].mean().reset_index()\n",
    "    variance_data = grouped_data[measurement_columns].std().reset_index()\n",
    "\n",
    "    # Calculate the number of rows in each group\n",
    "    count_data = grouped_data.size().reset_index(name='Count')\n",
    "\n",
    "    # Merge the mean, variance, and count data\n",
    "    mean_data = mean_data.merge(count_data, on=grouping_columns)\n",
    "    variance_data = variance_data.merge(count_data, on=grouping_columns)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Mean values for each group with counts:\\n\", mean_data)\n",
    "    print(\"\\nVariance values for each group with counts:\\n\", variance_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc3d46e-7818-4f71-9b91-81958ecf3333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_model(tp_measure=\"none\"):\n",
    "    topological_measure = tp_measure\n",
    "\n",
    "    (x, y, edge_index, train_mask, val_mask, test_mask) = get_dataset(\n",
    "        sparse=not dense, balanced=not make_unbalanced\n",
    "    )\n",
    "\n",
    "    num_features = x.shape[1]\n",
    "    num_classes = int(max(y) + 1)\n",
    "\n",
    "    # Construct networkx graph\n",
    "    G = construct_graph(x, y, edge_index, train_mask, val_mask, test_mask)\n",
    "\n",
    "    curvature = FormanRicci(G)\n",
    "    G_topo = G.copy()\n",
    "    adj_train_cond = None\n",
    "    adj_val_cond = None\n",
    "    adj_test_cond = None\n",
    "\n",
    "    # Split the graph to train, val, and test (inductive training)\n",
    "    (\n",
    "        x_train,\n",
    "        y_train,\n",
    "        edge_train,\n",
    "        train_mask,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        edge_val,\n",
    "        val_mask,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        edge_test,\n",
    "        test_mask,\n",
    "    ) = split_graph(G, multilabel=multilabel)\n",
    "\n",
    "    if topological_measure != \"none\":\n",
    "        graphs = []\n",
    "        stages = []\n",
    "        output_file = f\"./results/gnn/images/degree_distribution_{model_type}_topological_measure_{tp_measure}.png\"\n",
    "        G_old = G_topo\n",
    "\n",
    "        if topological_measure == \"curvature\":\n",
    "            curvature.compute_ricci_curvature()\n",
    "            G_topo = curvature.G\n",
    "            # Rename edge/node attribute 'formanCurvature' to 'topo'\n",
    "            for node in G_topo.nodes():\n",
    "                if \"formanCurvature\" in G_topo.nodes[node]:\n",
    "                    G_topo.nodes[node][\"topo\"] = G_topo.nodes[node].pop(\n",
    "                        \"formanCurvature\"\n",
    "                    )\n",
    "            for u, v in G_topo.edges():\n",
    "                if \"formanCurvature\" in G_topo[u][v]:\n",
    "                    G_topo[u][v][\"topo\"] = G_topo[u][v].pop(\"formanCurvature\")\n",
    "        elif topological_measure == \"degree_centrality\":\n",
    "            nodes_topo_results = nx.degree_centrality(G_topo)\n",
    "            edges_topo_results = edge_degree_centrality(G_topo)\n",
    "            for node in G_topo.nodes():\n",
    "                G_topo.nodes[node][\"topo\"] = nodes_topo_results[node]\n",
    "            for edge, degree_centrality in edges_topo_results.items():\n",
    "                G_topo[edge[0]][edge[1]][\"topo\"] = degree_centrality\n",
    "        elif topological_measure == \"betweenness_centrality\":\n",
    "            nodes_topo_results = nx.betweenness_centrality(G_topo)\n",
    "            edges_topo_results = nx.edge_betweenness_centrality(G_topo)\n",
    "            for node in G_topo.nodes():\n",
    "                G_topo.nodes[node][\"topo\"] = nodes_topo_results[node]\n",
    "            for u, v in G_topo.edges():\n",
    "                G_topo[u][v][\"topo\"] = edges_topo_results[(u, v)]\n",
    "        elif topological_measure == \"eigenvector_centrality\":\n",
    "            nodes_topo_results = nx.eigenvector_centrality(G_topo)\n",
    "            edges_topo_results = edge_metric_compute(\n",
    "                nodes_topo_results, G_topo\n",
    "            )\n",
    "            for node in G_topo.nodes():\n",
    "                G_topo.nodes[node][\"topo\"] = nodes_topo_results[node]\n",
    "            for edge, degree_centrality in edges_topo_results.items():\n",
    "                G_topo[edge[0]][edge[1]][\"topo\"] = degree_centrality\n",
    "        elif topological_measure == \"random\":\n",
    "            for node in G_topo.nodes():\n",
    "                G_topo.nodes[node][\"topo\"] = random.random()\n",
    "            for u, v in G_topo.edges():\n",
    "                G_topo[u][v][\"topo\"] = random.random()\n",
    "\n",
    "        print(f\"Number of edges before filtering: {G_topo.number_of_edges()}\")\n",
    "\n",
    "        if remove_edges > 0:\n",
    "            edge_topo = [(u, v, G_topo[u][v][\"topo\"]) for u, v in G_topo.edges()]\n",
    "            edge_topo.sort(key=lambda x: x[2])\n",
    "            edges_to_remove_high = edge_topo[-remove_edges:]\n",
    "            edges_to_remove_low = edge_topo[:remove_edges]\n",
    "            edges_to_remove = edges_to_remove_low\n",
    "            G_topo.remove_edges_from([(u, v) for u, v, _ in edges_to_remove])\n",
    "\n",
    "        print(f\"Number of edges after filtering: {G_topo.number_of_edges()}\")\n",
    "\n",
    "    # Normalize Adjacency Matrices\n",
    "    adj_train = construct_normalized_adj(edge_train, x_train.shape[0])\n",
    "    adj_val = construct_normalized_adj(edge_val, x_val.shape[0])\n",
    "    adj_test = construct_normalized_adj(edge_test, x_test.shape[0])\n",
    "\n",
    "    # Convert feature and labels to torch tensor\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    x_val = torch.tensor(x_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val)\n",
    "    val_mask = torch.tensor(val_mask)\n",
    "    x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test)\n",
    "    test_mask = torch.tensor(test_mask)\n",
    "\n",
    "    model = GAT(\n",
    "        hidden_channels=hidden_channels,\n",
    "        num_layers=layers,\n",
    "        in_channels=x_train.shape[1],\n",
    "        out_channels=num_classes,\n",
    "        batch_norm=batch_norm,\n",
    "        dropout=dropout,\n",
    "        residual=residual,\n",
    "    )\n",
    "\n",
    "    print(model)\n",
    "    torch.save(model, f\"./results/gnn/models/gat_{tp_measure}_model.pth\")\n",
    "    print(\"✅ Model saved successfully!\")\n",
    "\n",
    "    if do_evaluation:\n",
    "        (\n",
    "            max_val_acc,\n",
    "            max_val_f1,\n",
    "            max_val_sens,\n",
    "            max_val_spec,\n",
    "            max_val_test_acc,\n",
    "            max_val_test_f1,\n",
    "            max_val_test_sens,\n",
    "            max_val_test_spec,\n",
    "            session_memory,\n",
    "            train_memory,\n",
    "            train_time_avg,\n",
    "        ) = train(\n",
    "            model,\n",
    "            device,\n",
    "            x_train=x_train,\n",
    "            y_train=y_train,\n",
    "            adj_train=adj_train,\n",
    "            x_val=x_val,\n",
    "            y_val=y_val,\n",
    "            adj_val=adj_val,\n",
    "            val_mask=val_mask,\n",
    "            x_test=x_test,\n",
    "            y_test=y_test,\n",
    "            adj_test=adj_test,\n",
    "            test_mask=test_mask,\n",
    "            multilabel=multilabel,\n",
    "            lr=lr,\n",
    "            num_epoch=num_epoch,\n",
    "        )\n",
    "    else:\n",
    "        (\n",
    "            max_val_acc,\n",
    "            max_val_f1,\n",
    "            max_val_sens,\n",
    "            max_val_spec,\n",
    "            max_val_test_acc,\n",
    "            max_val_test_f1,\n",
    "            max_val_test_sens,\n",
    "            max_val_test_spec,\n",
    "            session_memory,\n",
    "            train_memory,\n",
    "            train_time_avg,\n",
    "        ) = train(\n",
    "            model,\n",
    "            device,\n",
    "            x_train=x_train,\n",
    "            y_train=y_train,\n",
    "            adj_train=adj_train,\n",
    "            x_val=None,\n",
    "            y_val=None,\n",
    "            adj_val=None,\n",
    "            val_mask=None,\n",
    "            x_test=None,\n",
    "            y_test=None,\n",
    "            adj_test=None,\n",
    "            test_mask=None,\n",
    "            multilabel=multilabel,\n",
    "            lr=lr,\n",
    "            num_epoch=num_epoch,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1080c8e5-6823-47ed-81e4-70b64d686378",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4091c4-da2b-4022-8476-6b12d79d0fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute_model(tp_measure=\"curvature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a61c93-9e34-45b8-af38-808bbd103876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute_model(tp_measure=\"degree_centrality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce0a8b4-09e3-4547-b776-82488d8976e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute_model(tp_measure=\"betweenness_centrality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161178e-f001-42d6-b2da-f9c197d55d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute_model(tp_measure=\"eigenvector_centrality\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
