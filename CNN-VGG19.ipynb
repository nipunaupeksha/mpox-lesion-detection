{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c6f04b-7742-4fde-ae1e-b9dea51d30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from keras_tuner import Hyperband\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, f1_score\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import cv2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce00d0-652f-44bc-8f27-04a19f8f7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/original'\n",
    "CNN_IMAGE_RESULTS = './results/cnn/images'\n",
    "CNN_HISTORY_RESULTS = './results/cnn/history'\n",
    "CNN_REPORT_RESULTS = './results/cnn/reports'\n",
    "CNN_MODEL_RESULTS = './results/cnn/models'\n",
    "\n",
    "TRAIN_DIRECTORY = 'train'\n",
    "VALIDATION_DIRECTORY = 'val'\n",
    "TEST_DIRECTORY = 'test'\n",
    "\n",
    "SUB_DIRECTORIES = [TRAIN_DIRECTORY, TEST_DIRECTORY, VALIDATION_DIRECTORY]\n",
    "CATEGORY_DIRECTORIES = ['Actinic keratoses', 'Basal cell carcinoma', 'Benign keratosis-like lesions', 'Chickenpox', 'Cowpox', 'Dermatofibroma', 'Healthy', 'HFMD', 'Measles', 'Melanocytic nevi', 'Melanoma', 'Monkeypox', 'Squamous cell carcinoma', 'Vascular lesions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aed7b7-a71f-44b4-b660-b3c04128f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(dir_name):\n",
    "    for dataset_type in SUB_DIRECTORIES:\n",
    "        total = 0\n",
    "        dir_type = os.path.join(dir_name, dataset_type)\n",
    "        print(f\"============ {dataset_type} dataset ===========\")\n",
    "        for category in os.listdir(dir_type):\n",
    "            category_path = os.path.join(dir_type, category)\n",
    "            if not os.path.isdir(category_path):\n",
    "                continue\n",
    "            images = [img for img in os.listdir(category_path) if img.endswith(('jpg','jpeg'))]\n",
    "            print(f\"Number of images in {category_path.split('/')[-1]}: {len(images)}\")\n",
    "            total += len(images)\n",
    "        print(f\"Total image count: {total}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df81e6-39a5-4e6c-b990-f8ace24e9a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_images(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d7dd5-1d08-44f2-8b35-7e48cec34263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_to_df(base_path, dataset_type):\n",
    "    path = os.path.join(base_path, dataset_type)\n",
    "    image_dir = Path(path)\n",
    "\n",
    "    file_paths = list(image_dir.glob(r'**/*.jpg'))\n",
    "    labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], file_paths))\n",
    "\n",
    "    file_paths = pd.Series(file_paths, name='Path').astype(str)\n",
    "    labels = pd.Series(labels, name='Label')\n",
    "\n",
    "    image_df = pd.concat([file_paths, labels], axis=1)\n",
    "\n",
    "    samples = []\n",
    "    for record in image_df['Label'].unique():\n",
    "        samples.append(image_df[image_df['Label']==record])\n",
    "    image_df = pd.concat(samples, axis=0).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5602be5-a61f-49e7-ba43-ee9480f033dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_images_to_df(DATA_PATH, TRAIN_DIRECTORY)\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ecc5c7-fda2-4d6a-9f84-c02d99974d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = load_images_to_df(DATA_PATH, VALIDATION_DIRECTORY)\n",
    "df_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82de354e-fb14-4841-8679-c3854919d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = load_images_to_df(DATA_PATH, TEST_DIRECTORY)\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ff632-1b5b-4518-b626-83e9eec97cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_plot_from_df(df, title):\n",
    "    pie = df[\"Label\"].value_counts()\n",
    "    pie.plot(kind=\"pie\", autopct=\"%1.2f%%\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.xlabel(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56807da5-d1a2-403d-86ed-edeedabd14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_from_df(df, nrows, ncols):\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15,8), subplot_kw={\"xticks\":[], \"yticks\":[]})\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(plt.imread(df.Path[i], -1))\n",
    "        ax.set_title(df.Label[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44a624-2698-4407-ba1b-da608368aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_plot_from_df(df_train, \"Image Percentages from Train Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e82aa-cd7a-4568-ac47-c3bb70783cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_plot_from_df(df_val, \"Image Percentages from Val Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd949b52-b14e-4206-bd3d-e48a99bf590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_plot_from_df(df_test, \"Image Percentages from Test Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54583b9d-e393-4a25-911f-31ff07409ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_from_df(df_train, 6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfc6457-18ca-49fc-843e-3ca811e8aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_from_df(df_val, 6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12adf985-de86-4974-a5a0-62864b88238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_from_df(df_test, 6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a27e1cd-3aed-4408-aacb-16977eb1b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.9, 1.1],\n",
    "    channel_shift_range=0.1,\n",
    "    fill_mode=\"nearest\",\n",
    "    preprocessing_function = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    ")\n",
    "\n",
    "val_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    ")\n",
    "\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e893a2c-85e5-42b2-bf61-bea6e9ef55ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(generator, df, subset):\n",
    "    images = generator.flow_from_dataframe(\n",
    "        dataframe=df,\n",
    "        x_col='Path', \n",
    "        y_col='Label',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        seed=42)\n",
    "    print(f\"{subset} class indices: {images.class_indices}\\n\")\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c896c85-4f07-4ce2-9960-2f4f254c38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_generator, df_train, TRAIN_DIRECTORY)\n",
    "val_dataset = create_dataset(val_generator, df_val, VALIDATION_DIRECTORY)\n",
    "test_dataset = create_dataset(test_generator, df_test, TEST_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c661a5-f990-40cc-a2df-7f6628cf321f",
   "metadata": {},
   "source": [
    "## Model Based on Random Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb122e7e-5645-4165-9662-1a4d2b063c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG19(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "inputs = base_model.input\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "outputs = Dense(len(CATEGORY_DIRECTORIES), activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "tf.keras.utils.plot_model(model, f\"{CNN_IMAGE_RESULTS}/vgg_19_model_summary.png\", show_shapes=True, dpi=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e8d5b-667e-459a-88ad-6c2c2fdaa5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(train_dataset.classes),\n",
    "    y=train_dataset.classes\n",
    ")\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=20,\n",
    "    mode=\"min\",\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695b386-759d-4503-addb-37e1baf07401",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=110,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e792b8f-71db-451d-b5bd-7c94019187df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], 'red', label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], 'green', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['categorical_accuracy'], 'orange', label='Training Accuracy')\n",
    "plt.plot(history.history['val_categorical_accuracy'], 'blue', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"{CNN_IMAGE_RESULTS}/vgg_19_training_plots.png\")\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_csv = f\"{CNN_HISTORY_RESULTS}/vgg_19_history.csv\"\n",
    "history_df.to_csv(history_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dffe01-5758-4ff7-a147-78cbf55d13e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_dataset)\n",
    "\n",
    "y_true = test_dataset.classes\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e7ccf-e159-42c9-9680-159f874cc974",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_true, y_pred, target_names=test_dataset.class_indices.keys())\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "report_file = f\"{CNN_REPORT_RESULTS}/vgg_19_classification_report.txt\"\n",
    "with open(report_file, 'w') as file:\n",
    "    file.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc2c19-d12e-4230-a013-ad0e7dc127d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='coolwarm', xticklabels=test_dataset.class_indices.keys(), yticklabels=test_dataset.class_indices.keys())\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "heatmap_file = f'{CNN_IMAGE_RESULTS}/vgg_19_confusion_matrix.png'\n",
    "plt.savefig(heatmap_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92b56e-f2e3-449d-83c2-350b99badb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"{CNN_MODEL_RESULTS}/vgg_19.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7814c-0a7e-437a-8c90-63a8dd614ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_test.sample(n=10, random_state=42)\n",
    "image_dict = dict(zip(df_sample[\"Path\"], df_sample[\"Label\"]))\n",
    "\n",
    "for path, label in image_dict.items():\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n",
    "\n",
    "    preds = model.predict(img_array)\n",
    "    predicted_class_idx = np.argmax(preds[0])\n",
    "\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    reversed_dict = {v: k for k, v in test_dataset.class_indices.items()}\n",
    "    predicted_class = reversed_dict[predicted_class_idx]\n",
    "    plt.title(f\"Original: {label} | Predicted: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8585789-4175-4eb3-b8d2-b63cfd0921b3",
   "metadata": {},
   "source": [
    "## Hyper-parameter Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645adb55-21ec-4ee8-b400-960698cd078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_with_hp(hp, base_model, num_classes):\n",
    "    inputs = base_model.input\n",
    "\n",
    "    x = base_model.output\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(units=hp.Int(\"dense_1_units\", min_value=128, max_value=512, step=32), activation=\"relu\")(x)\n",
    "    x = Dropout(rate=hp.Choice(\"dropout_rate_1\", values=[0.2, 0.3, 0.4, 0.5, 0.6, 0.7]))(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(units=hp.Int('dense_2_units', min_value=32, max_value=512, step=16), activation='relu')(x)\n",
    "\n",
    "    x = Dropout(rate=hp.Choice('dropout_rate_2', values=[0.2,0.3,0.4,0.5,0.6,0.7]))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "        ),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa19607-68ed-49ce-8040-547e65f3e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hyperparameter_tuning(model_name, base_model_func, train_dataset, val_dataset, num_classes, max_epochs=110, max_trials=5):\n",
    "    base_model = base_model_func(weights=\"imagenet\", include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "    # tuner = Hyperband(\n",
    "    #     lambda hp: build_model_with_hp(hp, base_model, num_classes),\n",
    "    #     objective=\"categorical_accuracy\",\n",
    "    #     max_epochs=max_epochs,\n",
    "    #     factor=3,\n",
    "    #     directory=\"hyperparameter_tuning\",\n",
    "    #     project_name=model_name\n",
    "    # )\n",
    "\n",
    "    tuner = RandomSearch(\n",
    "        lambda hp: build_model_with_hp(hp, base_model, num_classes),\n",
    "        objective=\"val_categorical_accuracy\",\n",
    "        max_trials=max_trials,  # Restrict to 5 trials\n",
    "        executions_per_trial=1,\n",
    "        directory=\"hyperparameter_tuning\",\n",
    "        project_name=model_name\n",
    "    )\n",
    "\n",
    "    tuner.search(train_dataset, validation_data=val_dataset, epochs=max_epochs, callbacks=[EarlyStopping(monitor='val_loss', patience=50, verbose=1, mode='min', restore_best_weights=True)])\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Best hyperparameters:\n",
    "    - Units in Dense Layer 1: {best_hps.get('dense_1_units')}\n",
    "    - Dropout Rate for Layer 1: {best_hps.get('dropout_rate_1')}\n",
    "    - Units in Dense Layer 2: {best_hps.get('dense_1_units')}\n",
    "    - Dropout Rate for Layer 2: {best_hps.get('dropout_rate_2')}\n",
    "    - Learning Rate: {best_hps.get('learning_rate')}\n",
    "    \"\"\")\n",
    "    \n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a95e6-8d11-401f-807c-375bb98528fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = perform_hyperparameter_tuning(\"VGG19\", VGG19, train_dataset, val_dataset, len(CATEGORY_DIRECTORIES), 5)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = build_model_with_hp(best_hps, base_model, len(CATEGORY_DIRECTORIES))\n",
    "tf.keras.utils.plot_model(model, f\"{CNN_IMAGE_RESULTS}/vgg_19_hp_model_summary.png\", show_shapes=True, dpi=50)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(train_dataset.classes),\n",
    "    y=train_dataset.classes\n",
    ")\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=110,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=50, verbose=1, mode='min', restore_best_weights=True)],\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c884c1-5041-44b2-ac2a-785603ebc727",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], 'red', label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], 'green', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['categorical_accuracy'], 'orange', label='Training Accuracy')\n",
    "plt.plot(history.history['val_categorical_accuracy'], 'blue', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"{CNN_IMAGE_RESULTS}/vgg_19_hp_training_plots.png\")\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_csv = f\"{CNN_HISTORY_RESULTS}/vgg_19_hp_history.csv\"\n",
    "history_df.to_csv(history_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77085942-dd85-4b18-a226-e869237db9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_dataset)\n",
    "\n",
    "y_true = test_dataset.classes\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b3b5d-37b2-4563-811e-6e68f49ce49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_true, y_pred, target_names=test_dataset.class_indices.keys())\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "report_file = f\"{CNN_REPORT_RESULTS}/vgg_19_hp_classification_report.txt\"\n",
    "with open(report_file, 'w') as file:\n",
    "    file.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57fcd31-fed7-48ee-bfaf-69bd09458555",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='coolwarm', xticklabels=test_dataset.class_indices.keys(), yticklabels=test_dataset.class_indices.keys())\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "heatmap_file = f'{CNN_IMAGE_RESULTS}/vgg_19_hp_confusion_matrix.png'\n",
    "plt.savefig(heatmap_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab707a4-4db9-4e90-92cf-e331e1ac685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"{CNN_MODEL_RESULTS}/vgg_19_hp.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8296dc-c08c-4de3-9959-466ae02d676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_test.sample(n=10, random_state=42)\n",
    "image_dict = dict(zip(df_sample[\"Path\"], df_sample[\"Label\"]))\n",
    "\n",
    "for path, label in image_dict.items():\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = tf.keras.applications.inception_v3.preprocess_input(img_array)\n",
    "\n",
    "    preds = model.predict(img_array)\n",
    "    predicted_class_idx = np.argmax(preds[0])\n",
    "\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    reversed_dict = {v: k for k, v in test_dataset.class_indices.items()}\n",
    "    predicted_class = reversed_dict[predicted_class_idx]\n",
    "    plt.title(f\"Original: {label} | Predicted: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
