{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3495e969-dffc-4bdb-b15a-52c4ca3b515a",
   "metadata": {},
   "source": [
    "# ü§ñ Use RNN Models to Identify MPox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf479c82-bc10-41ad-8990-0202b929f24c",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8812e6f-5a92-4868-ba56-d23f49d8caf6",
   "metadata": {},
   "source": [
    "The following code block imports all the libraries that are necessary for our development purposes.\n",
    "\n",
    "#### üî¢ `numpy`\n",
    "- NumPy is a powerful Python library used for numerical computing, providing support for large, multi-dimensional arrays, matrices, and high-level mathematical functions to operate on them efficiently.\n",
    "\n",
    "#### üêº `pandas`\n",
    "- Pandas is a Python library designed for data manipulation and analysis, offering easy-to-use data structures like DataFrames and Series for handling structured data efficiently.\n",
    "\n",
    "#### üë£ `pathlib`\n",
    "- Pathlib is a Python library that provides an object-oriented interface for working with filesystem paths, making path manipulation and file operations more intuitive and cross-platform.\n",
    "\n",
    "#### üìà `matplotlib`\n",
    "- Matplotlib is a Python library for creating static, interactive, and animated visualizations in a variety of formats, including plots, graphs, and charts.\n",
    "\n",
    "#### ‚ö° `tensorflow`\n",
    "- TensorFlow is an open-source library designed for building and deploying machine learning and deep learning models, offering a flexible ecosystem for numerical computation and AI development.\n",
    "\n",
    "#### üß† `keras`\n",
    "- Keras is a high-level deep learning library in Python that simplifies the creation and training of neural networks by providing an intuitive interface to underlying frameworks like TensorFlow.\n",
    "\n",
    "#### üåä `seaborn`\n",
    "- Seaborn is a Python library built on Matplotlib that simplifies creating aesthetically pleasing and informative statistical graphics for data visualization.\n",
    "\n",
    "#### üíª `os`\n",
    "- The os library in Python provides a way to interact with the operating system, enabling tasks such as file and directory manipulation, environment variable access, and process management.\n",
    "\n",
    "#### üî¨ `scikit-learn`\n",
    "- Scikit-learn is a Python library that provides simple and efficient tools for data mining, data analysis, and machine learning, including classification, regression, and clustering algorithms.\n",
    "\n",
    "#### üìÑ `shutil`\n",
    "- The shutil library in Python provides a suite of functions for high-level file operations, including copying, moving, renaming, and deleting files and directories.\n",
    "\n",
    "### üé≤ `random`\n",
    "- The random library in Python provides functions to generate random numbers, select random elements, and perform random operations like shuffling and sampling, useful for simulations, games, and probabilistic programming.\n",
    "\n",
    "### ‚ö†Ô∏è `warnings`\n",
    "- The warnings library in Python is used to issue, control, and filter warning messages during runtime, helping developers identify and address potential issues in their code without interrupting program execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c053e8d-353a-4eed-85f7-fb25a0262389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üóÇÔ∏è Import Libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras_tuner as kt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "\n",
    "### Supress warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf7cd06-ea4c-4b8d-a9a7-387d8f643836",
   "metadata": {},
   "source": [
    "## ‚ôæÔ∏è Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd76acc-aab1-485b-814c-ac4aa73d13ae",
   "metadata": {},
   "source": [
    "While proceeding with the process, we need some constants that may refer to the directory paths, image category names, data generation properties, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198652db-fc0d-4d1f-972d-f1490d91fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ‚ôæÔ∏è Constants\n",
    "\n",
    "### Directory paths\n",
    "DATA_PATH = './data/original'\n",
    "AUGMENTED_PATH = './data/rnn_augmented'\n",
    "RNN_BALANCED_PATH = './data/rnn_balanced'\n",
    "MODELS_PATH = './models/rnn'\n",
    "\n",
    "TRAIN_DIRECTORY='train'\n",
    "TEST_DIRECTORY='test'\n",
    "VALIDATION_DIRECTORY ='val'\n",
    "\n",
    "### Sample Values\n",
    "TRAIN_DATASET_SAMPLE_COUNT=1000\n",
    "TEST_DATASET_SAMPLE_COUNT=200\n",
    "VALIDATION_DATASET_SAMPLE_COUNT=200\n",
    "\n",
    "### Directories containing the images\n",
    "SUB_DIRECTORIES = [TRAIN_DIRECTORY, TEST_DIRECTORY, VALIDATION_DIRECTORY]\n",
    "DATA_DIRECTORIES = ['Actinic keratoses', 'Basal cell carcinoma', 'Benign keratosis-like lesions', 'Chickenpox', 'Cowpox', 'Dermatofibroma', 'Healthy', 'HFMD', 'Measles', 'Melanocytic nevi', 'Melanoma', 'Monkeypox', 'Squamous cell carcinoma', 'Vascular lesions']\n",
    "\n",
    "### Data generation properties\n",
    "ROTATION_RANGE = 40\n",
    "WIDTH_SHIFT_RANGE = 0.2\n",
    "HEIGHT_SHIFT_RANGE = 0.2\n",
    "SHEAR_RANGE = 0.2\n",
    "ZOOM_RANGE = 0.2\n",
    "HORIZONTAL_FLIP = True\n",
    "FILL_MODE = 'nearest'\n",
    "\n",
    "### Model-related constants\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "NUM_CLASSES = len(DATA_DIRECTORIES)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ceb38-04ac-42c7-807a-038cba66b567",
   "metadata": {},
   "source": [
    "## üßÆ Get Number of Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7733e96f-26fd-4ad9-9a06-1da8b0063d07",
   "metadata": {},
   "source": [
    "Get the number of images in each directory and sub-directory to get an understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc797496-a5e3-4900-b401-fba8aa7071de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üßÆ Get Number of Images\n",
    "\n",
    "### This method is used to get the number of images in each of the directories in our dataset\n",
    "\n",
    "def count_images(dir_name):\n",
    "    for dataset_type in SUB_DIRECTORIES:\n",
    "        dir_type = os.path.join(dir_name, dataset_type)\n",
    "        print(f\"{dataset_type}\")\n",
    "        print(\"----------------------------\")\n",
    "        for category in os.listdir(dir_type):\n",
    "            category_path = os.path.join(dir_type, category)\n",
    "            if not os.path.isdir(category_path):\n",
    "                continue\n",
    "            images = [img for img in os.listdir(category_path) if img.endswith(('jpg', 'jpeg'))]\n",
    "            print(f\"Number of images in {category_path.split('/')[-1]}: {len(images)}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd17377a-340c-4af3-9f7c-12d9d7161fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Execute the method\n",
    "\n",
    "count_images(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3005590-036c-42ab-8788-b76dcecd4f04",
   "metadata": {},
   "source": [
    "## üóúÔ∏è Image Augmentation for Categories with Low Image Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc21df-04a9-4e48-8bda-2917f8fca0cb",
   "metadata": {},
   "source": [
    "Since the image count is low on the following we need to augment those images. Then, we can add the original images and a set of augmented images for our model creation.\n",
    "\n",
    "***train*** Dataset (<1000)\n",
    "- *Cowpox*\n",
    "- *Actinic keratoses*\n",
    "- *Measels*\n",
    "- *Chickenpox*\n",
    "- *Squamous cell carcinoma*\n",
    "- *Dermatofibroma*\n",
    "- *Vascular lesions*\n",
    "\n",
    "***test*** Dataset (<200)\n",
    "- *Cowpox*\n",
    "- *Healthy*\n",
    "- *Actinic keratoses*\n",
    "- *Measels*\n",
    "- *Chickenpox*\n",
    "- *Squamous cell carcinoma*\n",
    "- *Dermatofibroma*\n",
    "- *Vascular lesions*\n",
    "\n",
    "***val*** Dataset (<200)\n",
    "- *Cowpox*\n",
    "- *Healthy*\n",
    "- *Actinic keratoses*\n",
    "- *Measels*\n",
    "- *Chickenpox*\n",
    "- *Squamous cell carcinoma*\n",
    "- *Dermatofibroma*\n",
    "- *Vascular lesions*\n",
    "\n",
    "### üíæ Define Data Generation Properties\n",
    "To augment images we need to use something called `ImageDataGenerator`. To use that we need to define data-gen properties. This way more data is generated according to the parameters we define. The parameters we will be using for this are as follows.\n",
    "\n",
    "- ‚Üª `rotation_range`\n",
    "    - Specifies the range(in degrees) within which the image is randomly rotated.\n",
    "    - For example, a value of `40` means the image can be rotated randomly by up to `40` degrees in either direction(clockwise or counterclockwise).<br><br>\n",
    "    \n",
    "- ‚Üî `width_shift_range`\n",
    "    - Specifies the fraction of the total width of the image by which it can be randomly shifted horizontally.\n",
    "    - For example, a value of `0.2` allows a horizontal shift of up to `20%` of the image's width.<br><br>\n",
    "    \n",
    "- ‚Üï `height_shift_range`\n",
    "    - Specifies the fraction of the total height of the image by which it can be randomly shifted vertically.\n",
    "    - For example, a value of `0.2` allows a vertical shift of up to `20%` of the image's height.<br><br>\n",
    "  \n",
    "- ‚û§ `shear_range`\n",
    "    - Specifies the intensity of shear transformation as a shear angle in a counterclockwise direction in degrees.\n",
    "    - A shear transformation distorts the image along an axis creating a parallelogram effect.<br><br>\n",
    "\n",
    "- üîé `zoom_range`\n",
    "   - Specifies the range for random zoom. It can zoom in or out within this range.\n",
    "   - For example, a value of `0.2` means the image size can be varied randomly by up to `¬±20%`.<br><br>\n",
    "\n",
    "- ‚Äï `horizontal_flip`\n",
    "   - Specifies whether the image should be flipped horizontally.\n",
    "   - If `True`, the image is randomly flipped along the horizontal axis, mirroring it.<br><br>\n",
    "\n",
    "- üö∞ `fill_mode`\n",
    "   - Specifies the strategy for filling in pixels that are introduced during transformations.\n",
    "   - The value `nearest` means the nearest pixel values are used to fill these gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce8ce48-d565-4e29-96c1-e49ffb7d1cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üóúÔ∏è Image Augmentation for Categories with Low Image Count\n",
    "\n",
    "### Define directories that lack the images count\n",
    "TRAIN_IMAGE_AUGEMENTATION_DIRS = ['Cowpox', 'Actinic keratoses', 'Measles', 'Chickenpox', 'Squamous cell carcinoma', 'Dermatofibroma', 'Vascular lesions']\n",
    "TEST_IMAGE_AUGEMENTATION_DIRS = ['Cowpox', 'Healthy', 'Actinic keratoses', 'Measles', 'Chickenpox', 'Squamous cell carcinoma', 'Dermatofibroma', 'Vascular lesions']\n",
    "VAL_IMAGE_AUGEMENTATION_DIRS = ['Cowpox', 'Healthy', 'Actinic keratoses', 'Measles', 'Chickenpox', 'Squamous cell carcinoma', 'Dermatofibroma', 'Vascular lesions']\n",
    "\n",
    "### Create Image Generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=ROTATION_RANGE,\n",
    "    width_shift_range=WIDTH_SHIFT_RANGE,\n",
    "    height_shift_range=HEIGHT_SHIFT_RANGE,\n",
    "    shear_range=SHEAR_RANGE,\n",
    "    zoom_range=ZOOM_RANGE,\n",
    "    horizontal_flip=HORIZONTAL_FLIP,\n",
    "    fill_mode=FILL_MODE)\n",
    "\n",
    "### Function to process and augment data\n",
    "def augment_and_copy_data(dir_name):\n",
    "    for dataset_type in SUB_DIRECTORIES:\n",
    "        dir_type = os.path.join(dir_name, dataset_type)\n",
    "        #### Dependeing on the dataset_type pick the categories with less number of images\n",
    "        categories = []\n",
    "        if dataset_type == TRAIN_DIRECTORY:\n",
    "            categories = TRAIN_IMAGE_AUGEMENTATION_DIRS\n",
    "        elif dataset_type == TEST_DIRECTORY:\n",
    "            categories = TEST_IMAGE_AUGEMENTATION_DIRS\n",
    "        else:\n",
    "            categories = VAL_IMAGE_AUGEMENTATION_DIRS\n",
    "        for category in categories:\n",
    "            category_path = os.path.join(dir_type, category)\n",
    "            #### Get augmented dataset path (e.g. ./data/augmented/train)\n",
    "            augmented_path = os.path.join(AUGMENTED_PATH, dataset_type)\n",
    "            #### Create the dataset type directory if it does not exist\n",
    "            os.makedirs(augmented_path, exist_ok=True)\n",
    "            #### Get augmented category path (e.g. ./data/augmented/train/Cowpox)\n",
    "            augmented_category_path = os.path.join(augmented_path, category)\n",
    "            #### Create a directory if it does not exist\n",
    "            os.makedirs(augmented_category_path, exist_ok=True)\n",
    "\n",
    "            #### Get all the files inside a category directory\n",
    "            files = os.listdir(category_path)\n",
    "\n",
    "            print(f'Started augmenting images in: {category_path}')\n",
    "\n",
    "            #### Iterate files\n",
    "            for file in files:\n",
    "                if file.startswith('.'): ### Skip hidden files\n",
    "                    continue\n",
    "                #### Load images\n",
    "                img_path = os.path.join(category_path, file)\n",
    "                img = load_img(str(img_path))\n",
    "                x = img_to_array(img) #### This is a NumPy array with shape (3, 150, 150)\n",
    "                x = x.reshape((1,) + x.shape) #### This is a NumPy array with shape (1, 3, 150, 150)\n",
    "                #### The .flow() command generates batches of randomly transformed images and saves the results in the ./data/augmented/<sub_directory>/<data_directory> along with the original image\n",
    "                #### print(f'Started augmenting image: {img_path}')\n",
    "                i = 0\n",
    "                for batch in datagen.flow(x, batch_size=1, save_to_dir=augmented_category_path, save_prefix=file.split(\".\")[0], save_format='jpg'):\n",
    "                    i += 1\n",
    "                    if i > 10:\n",
    "                        break #### Stop looping the generator infinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ece86-f187-423d-963c-4761ae71a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Execute the method\n",
    "\n",
    "augment_and_copy_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f21f0db-08e0-475a-bb3a-5b06b9275df6",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Create a Balanced Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216fb583-660b-45da-a051-add2b2607aa8",
   "metadata": {},
   "source": [
    "We need to create a balanced dataset to use the RNN models more effectively. The lowest number of images in category directories of the `train` dataset is 191. Therefore, we can create a balanced dataset containing 191 images from each category directory.\n",
    "\n",
    "To do that we can create a method named `create_balanced_dataset` with three inputs, `source_dir`, `dest_dir`, and `num_images`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c567f-1281-4b34-8359-528bca8dd007",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ‚öñÔ∏è Create a Balanced Dataset\n",
    "### This method is used to copy a certain number of images from every, train, test, and validation set.\n",
    "\n",
    "def limit_data_from_source(source_dir, dest_dir, num_images=TRAIN_DATASET_SAMPLE_COUNT):\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "    for dataset_type in SUB_DIRECTORIES:\n",
    "        source_type_dir = os.path.join(source_dir, dataset_type)\n",
    "        dest_type_dir = os.path.join(dest_dir, dataset_type)\n",
    "        os.makedirs(dest_type_dir, exist_ok=True)\n",
    "        for category in os.listdir(source_type_dir):\n",
    "            category_path = os.path.join(source_type_dir, category)\n",
    "            if not os.path.isdir(category_path):\n",
    "                continue\n",
    "            dest_category_path = os.path.join(dest_type_dir, category)\n",
    "            os.makedirs(dest_category_path, exist_ok=True)\n",
    "\n",
    "            images = [img for img in os.listdir(category_path) if img.endswith(('jpg', 'jpeg'))]\n",
    "            print(f\"Number of images in {dataset_type}/{category_path.split('/')[-1]}: {len(images)}\")\n",
    "            #### Select the number of images provided in num_images param or the minimum number of images\n",
    "            if dataset_type == TRAIN_DIRECTORY:\n",
    "                num_images = TRAIN_DATASET_SAMPLE_COUNT\n",
    "            elif dataset_type == TEST_DIRECTORY:\n",
    "                num_images = TEST_DATASET_SAMPLE_COUNT\n",
    "            else:\n",
    "                num_images = VALIDATION_DATASET_SAMPLE_COUNT\n",
    "            selected_images = random.sample(images, min(num_images, len(images)))\n",
    "\n",
    "            for image in selected_images:\n",
    "                source_image_path = os.path.join(category_path, image)\n",
    "                dest_image_path = os.path.join(dest_category_path, image)\n",
    "                #### print(f'Copying {source_image_path} to {dest_image_path}')\n",
    "                shutil.copy(source_image_path, dest_image_path)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd1c3e-d631-4f50-81e5-8cc8b108f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_data_from_source(DATA_PATH, RNN_BALANCED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49873d3b-0ac4-4f05-8fa3-1e3dbfa5f985",
   "metadata": {},
   "source": [
    "## ‚ûï Add the Augmented Images to the Balanced Dataset to Balance the Number of Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cff50d-41ea-44d1-aa7d-15f507c474c2",
   "metadata": {},
   "source": [
    "The next step is to identify the number of missing images from the balanced dataset and add the missing images from the augmented images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d332b1a-a13f-4b68-b2b0-af107c3e316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_images(RNN_BALANCED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1269ae-e8bf-46fb-a6e4-b5a95c228fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since there are missing images, we need to find out exactly how many images we are missing by creating dictiionaries for each of the test, train, and val directories.\n",
    "## Function for getting missing image count for train, test, and val\n",
    "\n",
    "def get_missing_images_count(dir_name):\n",
    "    ## Create dictionaries for train, test, and validation\n",
    "    train_dict = dict()\n",
    "    test_dict = dict()\n",
    "    val_dict = dict()\n",
    "    for dataset_type in SUB_DIRECTORIES:\n",
    "        dir_type = os.path.join(dir_name, dataset_type)\n",
    "        for category in os.listdir(dir_type):\n",
    "            category_path = os.path.join(dir_type, category)\n",
    "            if not os.path.isdir(category_path):\n",
    "                continue\n",
    "            images = [img for img in os.listdir(category_path) if img.endswith(('jpg', 'jpeg'))]\n",
    "            required_count = 0\n",
    "            if dataset_type == TRAIN_DIRECTORY:\n",
    "                image_count = min(len(images), TRAIN_DATASET_SAMPLE_COUNT)\n",
    "                required_count = TRAIN_DATASET_SAMPLE_COUNT - image_count\n",
    "                train_dict[category_path.split(\"/\")[-1]] = required_count\n",
    "            elif dataset_type == TEST_DIRECTORY:\n",
    "                image_count = min(len(images), TEST_DATASET_SAMPLE_COUNT)\n",
    "                required_count = TEST_DATASET_SAMPLE_COUNT - image_count\n",
    "                test_dict[category_path.split(\"/\")[-1]] = required_count\n",
    "            else:\n",
    "                image_count = min(len(images), VALIDATION_DATASET_SAMPLE_COUNT)\n",
    "                required_count = VALIDATION_DATASET_SAMPLE_COUNT - image_count\n",
    "                val_dict[category_path.split(\"/\")[-1]] = required_count\n",
    "    return train_dict, test_dict, val_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1477246-c6ef-41f6-80c9-99d84b5c440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ‚ûï Add the Augmented Images to the Balanced Dataset to Balance the Number of Images\n",
    "\n",
    "## Get the missing number of images\n",
    "train_dict, test_dict, val_dict = get_missing_images_count(RNN_BALANCED_PATH)\n",
    "\n",
    "## Print the missing values\n",
    "\n",
    "def print_dict(dict_name, dataset_type):\n",
    "    print(f\"Missing values in {dataset_type}\")\n",
    "    print(\"----------------------------\")\n",
    "    for key, value in train_dict.items():\n",
    "        print(f\"{key} : {value}\")\n",
    "\n",
    "### print train_dict key, value pairs\n",
    "print_dict(train_dict, TRAIN_DIRECTORY)\n",
    "\n",
    "### print test_dict key, value pairs\n",
    "print_dict(test_dict, TEST_DIRECTORY)\n",
    "\n",
    "### print val_dict key, value pairs\n",
    "print_dict(val_dict, VALIDATION_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467dafbe-d8cb-4d40-9e07-0f6d96b57e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to copy the missing number of images randomly from the augmented dataset\n",
    "\n",
    "def copy_missing_images_from_augmented(src_path, dest_path):\n",
    "    # Get the missing image count\n",
    "    train_dict, test_dict, val_dict = get_missing_images_count(RNN_BALANCED_PATH)\n",
    "\n",
    "    # Dictionary mapping dataset types to their corresponding missing counts\n",
    "    missing_counts = {\n",
    "        TRAIN_DIRECTORY: train_dict,\n",
    "        TEST_DIRECTORY: test_dict,\n",
    "        VALIDATION_DIRECTORY: val_dict,\n",
    "    }\n",
    "\n",
    "    for dataset_type in SUB_DIRECTORIES:\n",
    "        src_dir_type = os.path.join(src_path, dataset_type)\n",
    "        dest_dir_type = os.path.join(dest_path, dataset_type)\n",
    "\n",
    "        # Pass the dictionary for the current dataset type\n",
    "        copy_images_for_dataset_type(src_dir_type, dest_dir_type, missing_counts[dataset_type])\n",
    "\n",
    "## Helper function to copy images for a specific dataset type\n",
    "\n",
    "def copy_images_for_dataset_type(src_dir, dest_dir, missing_dict):\n",
    "    for key, value in missing_dict.items():\n",
    "        if value == 0:\n",
    "            continue\n",
    "        src_cat_dir = os.path.join(src_dir, key)\n",
    "        dest_cat_dir = os.path.join(dest_dir, key)\n",
    "\n",
    "        # Get all available images in the source category directory\n",
    "        images = [img for img in os.listdir(src_cat_dir) if img.endswith(('jpg', 'jpeg'))]\n",
    "\n",
    "        # Randomly select the missing number of images\n",
    "        selected_images = random.sample(images, min(value, len(images)))\n",
    "\n",
    "        # Copy the selected images to the destination directory\n",
    "        for img in selected_images:\n",
    "            shutil.copy(os.path.join(src_cat_dir, img), dest_cat_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55e0de-f822-4640-8aa6-565ac13b8564",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copy the missing images from augmented images\n",
    "\n",
    "copy_missing_images_from_augmented(AUGMENTED_PATH, RNN_BALANCED_PATH)\n",
    "\n",
    "## Count the number of images again\n",
    "\n",
    "train_dict, test_dict, val_dict = get_missing_images_count(RNN_BALANCED_PATH)\n",
    "\n",
    "### print train_dict key, value pairs\n",
    "print_dict(train_dict, TRAIN_DIRECTORY)\n",
    "\n",
    "### print test_dict key, value pairs\n",
    "print_dict(test_dict, TEST_DIRECTORY)\n",
    "\n",
    "### print val_dict key, value pairs\n",
    "print_dict(val_dict, VALIDATION_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a198b9e5-b4b4-4451-8c95-0b5e2a657660",
   "metadata": {},
   "source": [
    "## üóÉÔ∏è Creating File Data Frame for Train Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b4b58-0cf8-4795-99cc-09ea001dc2e4",
   "metadata": {},
   "source": [
    "Now, we have all the required images to make our RNN model.\n",
    "\n",
    "Next, to make our RNN model, we need to create a data frame that will contain the paths and the names of the dermatological diseases. \n",
    "\n",
    "After creating the data frame, we can check whether there is a data imbalance with a pie chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7acda-279f-4f0a-89cf-711ef9a6bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üóÉÔ∏è Creating File Data Frame for Train Dataset\n",
    "\n",
    "def load_images_to_df(base_path, dataset_type, sample_size):\n",
    "    ### Load the base directory path\n",
    "    path = os.path.join(base_path, dataset_path)\n",
    "    image_dir = Path(path)\n",
    "\n",
    "    ### Get file paths and assign labels\n",
    "    file_paths = list(image_dir.glob(r'**/*.jpg'))\n",
    "    labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], file_paths))\n",
    "\n",
    "    file_paths = pd.Series(file_paths, name='Path').astype(str)\n",
    "    labels = pd.Series(labels, name='Label')\n",
    "\n",
    "    ### Concatenate file paths and labels\n",
    "    image_df = pd.concat([file_paths, labels], axis=1)\n",
    "\n",
    "    ### Get samples according to the sample size\n",
    "    samples =[]\n",
    "    for record in image_df['Label'].unique():\n",
    "        samples.append(image_df[image_df['Label'] == record].sample(sample_size, random_state=42))\n",
    "    image_df = pd.concat(samples, axis=0).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    ### Return the data frame\n",
    "    return image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd29e36-64c2-45fb-9012-0271168a4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the images of the train set\n",
    "image_df_train = load_images_to_df(RNN_BALANCED_PATH, TRAIN_DIRECTORY, TRAIN_DATASET_SAMPLE_COUNT)\n",
    "\n",
    "### Show output\n",
    "image_df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da62c1-eca5-4b6f-919f-2caf9cd656ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the number of images to show there are no biases in the image set\n",
    "\n",
    "pie = image_df_train['Label'].value_counts()\n",
    "pie.plot(kind='pie', autopct='%1.2f%%')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a65e7e-a230-4328-b7b5-ed4087b4286f",
   "metadata": {},
   "source": [
    "## üñ•Ô∏è Display Images of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a167f93-4c66-4323-acaf-7525ae89a6c4",
   "metadata": {},
   "source": [
    "Since we see that there is no data imbalance, the next step is to see whether our data has been loaded correctly or not in our data frame.\n",
    "\n",
    "We can check that by plotting a sample of our images. To do that, we can use the `matplotlib` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbf80e-aefe-4513-af1d-1a78ff151a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üñ•Ô∏è Display Images of the Dataset\n",
    "\n",
    "### Define figure and axes\n",
    "fig, axes = plt.subplots(nrows=6, ncols=5, figsize=(15,8), subplot_kw={'xticks':[], 'yticks':[]})\n",
    "\n",
    "### Display 30 images of the dataset\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(plt.imread(image_df_train.Path[i], -1))\n",
    "    ax.set_title(image_df_train.Label[i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9717323-d22f-42a3-90bd-2c633f297cab",
   "metadata": {},
   "source": [
    "## üóÉÔ∏è Load the Test and Validation Data Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92662d1f-bc19-4bfd-971f-7b8d45f915ac",
   "metadata": {},
   "source": [
    "Next, load the test and validation data frames so that we can use them to create the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6487c160-62ec-48b8-9e39-f66f10b38846",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üóÉÔ∏è Load the Test and Validation Data Frames\n",
    "\n",
    "image_df_test = load_images_to_df(RNN_BALANCED_PATH, TEST_DIRECTORY, TEST_DATASET_SAMPLE_COUNT)\n",
    "image_df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81ad15-ffbc-4a46-bf03-4e7e2b8e338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df_val = load_images_to_df(RNN_BALANCED_PATH, VALIDATION_DIRECTORY, VALIDATION_DATASET_SAMPLE_COUNT)\n",
    "image_df_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914581df-2a55-44b8-925b-7eded3a24b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(df, img_height=IMG_HEIGHT, img_width=IMG_WIDTH):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        img = tf.keras.utils.load_img(row['Path'], target_size=(img_height, img_width))\n",
    "        img_array = tf.keras.utils.img_to_array(img)/255.0 # Normalize\n",
    "        img_array = img_array.reshape(img_height, -1) # Flatten into rows(sequential data)\n",
    "        images.append(img_array)\n",
    "        labels.append(row['Label'])\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cdc555-d05b-4aac-a2bd-f09d493b584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_images, train_labels = preprocess_images(image_df_train)\n",
    "val_images, val_labels = preprocess_images(image_df_val)\n",
    "test_images, test_labels = preprocess_images(image_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b0f84-379f-4e4e-919c-789f0dc424c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "val_labels = label_encoder.transform(test_labels)\n",
    "test_labels = label_encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4568a0e5-692e-42bf-b6e3-bc3bbde8a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "train_labels = to_categorical(train_labels, NUM_CLASSES)\n",
    "val_labels = to_categorical(val_labels, NUM_CLASSES)\n",
    "test_labels = to_categorical(test_labels, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfd1436-a7a4-4d6e-9bf7-15befe76c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_lstm(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # First LSTM layer with tunable number of units\n",
    "    model.add(LSTM(units=hp.Int('units_lstm_1', min_value=64, max_value=256, step=64),\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH *3),\n",
    "    return_sequences=True))\n",
    "\n",
    "    # Second LSTM layer with tunable number of units\n",
    "    model.add(LSTM(units=hp.Int('units_lstm_2', min_value=32, max_value=128, step=32)))\n",
    "\n",
    "    # Dropout layer with tunable rate\n",
    "    moel.add(Dropout(hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Dense layer with tunable number of units\n",
    "    model.add(Dense(units=hp.Int('units_dense', min_value=16, max_value=64, step=16), activation='relu'))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.legacy.Adam(\n",
    "            learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "        ),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1265d2ad-db57-4d95-918b-a511e124a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=20,\n",
    "    factor=3,\n",
    "    directory='rnn_tuning',\n",
    "    project_name='image_classification'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d749079-5585-408b-adfb-138c7487cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an early stopping callback\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5304632-1b7f-4cb6-945a-b9bc67105792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hyperparameter search\n",
    "tuner.search(\n",
    "    train_images, train_labels,\n",
    "    validation_data=(val_images, val_labels),\n",
    "    epochs=20,\n",
    "    callbacks=[stop_early],\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96fe71f-48f6-4a39-85e9-07188c1443e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters and model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "Best hyperparameters:\n",
    "- Units in LSTM Layer 1: {best_hps.get('units_lstm_1')}\n",
    "- Units in LSTM Layer 2: {best_hps.get('units_lstm_2')}\n",
    "- Dropout Rate: {best_hps.get('dropout_rate')}\n",
    "- Units in Dense Layer: {best_hps.get('units_dense')}\n",
    "- Learning Rate: {best_hps.get('learning_rate')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a29dc8-1f21-4cb5-a7e8-5b87d9643cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the best model and retrain\n",
    "best_model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a0110-2f02-41cb-9fdc-b95ac7660c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_model.fit(train_images, train_labels, validation_data=(val_images, val_labels),\n",
    "                    batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a4bee-99b7-4d2f-8efd-41648e1e6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = best_model.evaluate(test_images, test_labels)\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b530d-f12e-4bdd-85e4-f19f7472467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "predictions = best_model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(test_labels, axis=1)\n",
    "print(classification_report(true_classes, predicted_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a0d24-d28a-4622-8c5e-c508b311ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Plot\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0704a5df-8055-49a3-bb4c-342286999f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Plot\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a209aa4-24b4-4278-b63d-1c3d5211a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    ### First GRU layer with tunable number of units\n",
    "    model.add(GRU(units=hp.Int('units_gru_1', min_value=64, max_value=256, step=64), input_shape=(IMG_HEIGHT, IMG_WIDTH * 3), return_sequences=True))\n",
    "\n",
    "    ### Second GRU layer with tunable number of units\n",
    "    model.add(GRU(units=hp.Int('units_gru_2', min_value=32, max_value=128, step=32)))\n",
    "\n",
    "    ### Dropout layer with tunable rate\n",
    "    model.add(Dropout(hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "    ### Dense layer with tunable number of units\n",
    "    model.add(Dense(units=hp.Int('units_dense', min_value=16, max_value=64, step=16), activation='relu'))\n",
    "\n",
    "    ### Output layer\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "    ### Compile the model with tunable learning rate\n",
    "    model.compile(optimizer = tf.keras.optimizers.legacy.Adam(\n",
    "        learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    ))\n",
    "\n",
    "    ### Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b532e82-c106-4a0d-bb18-1d137387f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize the tuner for GRU\n",
    "tuner = kt.Hyperband(\n",
    "    build_gru_model,\n",
    "    objectives='val_accuracy',\n",
    "    max_epochs=20,\n",
    "    factor=3,\n",
    "    directory='gru_tuning',\n",
    "    project_name='image_classification_gru'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ffef56-6b54-4a70-a4d6-df9c8e5bd10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Early stopping to avoid overfitting\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042be99-c6a8-4c68-baed-87ae8bfade52",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the hyperparameter search\n",
    "tuner.search(\n",
    "    train_images, train_labels,\n",
    "    validation_data=(val_images, val_labels),\n",
    "    epochs=20,\n",
    "    callbacks=[stop_early],\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e648e1-9b86-4671-9943-902d6f1ee195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best hyperparameters and model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "Best hyperparameters:\n",
    "- Units in GRU Layer 1: {best_hps.get('units_gru_1')}\n",
    "- Units in GRU Layer 2: {best_hps.get('units_gru_2')}\n",
    "- Dropout Rate: {best_hps.get('dropout_rate')}\n",
    "- Units in Dense Layer: {best_hps.get('units_dense')}\n",
    "- Learning Rate: {best_hps.get('learning_rate')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6088b29-3fbc-4888-a908-0e1ffd31667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the best GRU model and retrain it\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(\n",
    "    train_images, train_labels,\n",
    "    validation_data=(val_images, val_labels),\n",
    "    epochs=20,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[stop_early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b77ce4-f6c1-4558-b329-7676038eca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best GRU model\n",
    "test_loss, test_accuracy = best_model.evaluate(test_images, test_labels)\n",
    "print(f\"Test Accuracy with Best Hyperparameters: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef08ae7-213e-4acc-a1c7-f165f3a28b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "predictions = best_model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(test_labels, axis=1)\n",
    "print(classification_report(true_classes, predicted_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0a628-9709-48a0-913d-92ec26b0778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy plot\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Model Accuracy (GRU)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9652456d-25cd-4ba1-a8b0-e4605bdb8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss (GRU)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
