{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e98fb86-748e-487f-ad93-7c5ae315a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics as sk\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import networkx as nx\n",
    "from torch_geometric.typing import SparseTensor\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb92223-8a2b-4b95-8097-526f838dabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGES_PATH = './data/images/train'\n",
    "TEST_IMAGES_PATH = './data/images/test'\n",
    "VAL_IMAGES_PATH = './data/images/val'\n",
    "\n",
    "TRAIN_NPZ_FILE = './data/npz/train_images.npz'\n",
    "TEST_NPZ_FILE = './data/npz/test_images.npz'\n",
    "VAL_NPZ_FILE = './data/npz/val_images.npz'\n",
    "\n",
    "NUM_FEATURES = 224 * 224 * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ef908-8888-45f3-8ccc-9087f3198620",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "963478c4-d010-4f23-bcba-d9ad86e71429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz_as_tensors(file_path):\n",
    "    \"\"\"\n",
    "    Load the .npz files as tensors\n",
    "\n",
    "    Args:\n",
    "        file_path (string): To get the .npz file and load it as a tensor\n",
    "    \"\"\"\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "    images = data['images']\n",
    "    labels = data['labels']\n",
    "\n",
    "    images_tensor = torch.tensor(images, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return images_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd73e6-723e-4e50-b41f-51a40596f7a9",
   "metadata": {},
   "source": [
    "## Process Data and Save Masked Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f19001-e79a-4f13-a8b2-180e7621bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(sparsity):\n",
    "    \"\"\"\n",
    "    Process and save the data in both sparse and dense formats.\n",
    "\n",
    "    Args:\n",
    "        sparsity (bool): Whether to process the dataset as sparse or dense\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load training data\n",
    "    train_images, train_labels = load_npz_as_tensors(TRAIN_NPZ_FILE)\n",
    "    train_data = train_images.reshape(train_images.shape[0], NUM_FEATURES)\n",
    "    num_train = train_images.shape[0]\n",
    "\n",
    "    # Load validation data\n",
    "    val_images, val_labels = load_npz_as_tensors(VAL_NPZ_FILE)\n",
    "    val_data = val_images.reshape(val_images.shape[0], NUM_FEATURES)\n",
    "    num_val = val_images.shape[0]\n",
    "\n",
    "    # Load test data\n",
    "    test_images, test_labels = load_npz_as_tensors(TEST_NPZ_FILE)\n",
    "    test_data = test_images.reshape(test_images.shape[0], NUM_FEATURES)\n",
    "    num_test = test_images.shape[0]\n",
    "\n",
    "    # Concatenate train, validation, and test data\n",
    "    num_data = num_train + num_val + num_test\n",
    "    data_feat = np.concatenate((train_data, val_data, test_data), axis=0)\n",
    "    data_label = np.concatenate((train_labels, val_labels, test_labels), axis=0).reshape(-1)\n",
    "\n",
    "    # Construct and scale adjacency matrix\n",
    "    adj_matrix = sk.pairwise.cosine_similarity(data_feat, data_feat)\n",
    "    adj_matrix = (adj_matrix - adj_matrix.min())/(adj_matrix.max()-adj_matrix.min())\n",
    "\n",
    "    # Apply sparsity thresholds\n",
    "    threshold = 0.977 if sparsity else 0.970\n",
    "\n",
    "    adj_matrix = adj_matrix > threshold\n",
    "\n",
    "    # Generate masks\n",
    "    train_mask = np.zeros(num_data, dtype=bool)\n",
    "    train_mask[:num_train] = True\n",
    "    val_mask = np.zeros(num_data, dtype=bool)\n",
    "    val_mask[num_train:num_train + num_val] = True\n",
    "    test_mask = np.zeros(num_data, dtype=bool)\n",
    "    test_mask[num_train + num_val:] = True\n",
    "\n",
    "    # Save masks, features, labels, and edge index\n",
    "    suffix = 'sparse' if sparsity else 'dense'\n",
    "    base_path = f\"./data/npz/{suffix}\"\n",
    "\n",
    "    np.savez_compressed(f\"{base_path}/train_mask.npz\", train_mask=train_mask)\n",
    "    np.savez_compressed(f\"{base_path}/val_mask.npz\", val_mask=val_mask)\n",
    "    np.savez_compressed(f\"{base_path}/test_mask.npz\", test_mask=test_mask)\n",
    "    np.savez_compressed(f\"{base_path}/data_feat.npz\", data_feat=data_feat)\n",
    "    np.savez_compressed(f\"{base_path}/data_label.npz\", data_label=data_label)\n",
    "\n",
    "    # Generate and save edge index\n",
    "    edge_index = np.array([[i, j] for i in range(num_data) for j in range(num_data) if i != j and adj_matrix[i, j]])\n",
    "    np.savez_compressed(f\"{base_path}/edge_index.npz\", edge_index=edge_index)\n",
    "\n",
    "    print(f\"View-({'Sparse' if sparsity else 'Dense'}) generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b9194-f73d-4ec6-8391-3ab695d8c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with sparsity parameter\n",
    "process_data(sparsity=True)\n",
    "process_data(sparsity=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a9db9-0c40-4643-9dd8-efd158dd652b",
   "metadata": {},
   "source": [
    "## Load Data, Masks, and Print Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac277c1-033f-4b77-b556-8f3ddc3b7fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics(features, labels, edge_index, train_mask, val_mask, test_mask):\n",
    "    \"\"\"\n",
    "    Print statistics of the dataset\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Node features\n",
    "        labels (np.ndarray): Node labels\n",
    "        edge_index (np.ndarray): Edge indices\n",
    "        train_mask (torch.Tensor): Mask for training nodes\n",
    "        val_mask (torch.Tensor): Mask for validation nodes\n",
    "        test_mask (torch.Tensor): Mask for test nodes\n",
    "    \"\"\"\n",
    "    print(\"=============== Dataset Types ==============\")\n",
    "    print(f\"Type of features: {type(features)}\")\n",
    "    print(f\"Type of labels: {type(labels)}\")\n",
    "    print(f\"Type of edge_index: {type(edge_index)}\")\n",
    "    print(f\"Type of train_mask: {type(train_mask)}\")\n",
    "    print(f\"Type of val_mask: {type(val_mask)}\")\n",
    "    print(f\"Type of test_mask: {type(test_mask)}\")\n",
    "    print(\"=============== Dataset Properties ==================\")\n",
    "    print(f\"Total Nodes: {features.shape[0]}\")\n",
    "    print(f\"Total Edges: {edge_index.shape[0]}\")\n",
    "    print(f\"Number of Features: {features.shape[1]}\")\n",
    "    if labels.ndim == 1:\n",
    "        print(f\"Number of Labels: {labels.max() + 1}\")\n",
    "        print(\"Task Type: Multi-class Classification\")\n",
    "    else:\n",
    "        print(f\"Number of Labels: {labels.shape[1]}\")\n",
    "        print(\"Task Type: Multi-label Classification\")\n",
    "    print(f\"Training Nodes: {train_mask.sum().item()}\")\n",
    "    print(f\"Validation Nodes: {val_mask.sum().item()}\")\n",
    "    print(f\"Testing Nodes: {test_mask.sum().item()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27431e-7abd-45e7-a7a0-4549b3bde472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(sparse=True, balanced=True):\n",
    "    \"\"\"\n",
    "    Load the dataset in either sparse or dense format\n",
    "\n",
    "    Args:\n",
    "        sparse (bool): Whether to load the sparse or dense version of the dataset\n",
    "    \"\"\"\n",
    "    print(f\"Loading Dataset\")\n",
    "    \n",
    "    # Load masks\n",
    "    suffix = 'sparse' if sparse else 'dense'\n",
    "    train_mask = torch.tensor(np.load(f\"./data/npz/{suffix}/train_mask.npz\"))\n",
    "    val_mask = torch.tensor(np.load(f\"./data/npz/{suffix}/val_mask.npz\"))\n",
    "    test_mask = torch.tensor(np.load(f\"./data/npz/{suffix}/test_mask.npz\"))\n",
    "\n",
    "    # Load labels\n",
    "    labels = np.load(f\"./data/npz/{suffix}/data_label.npz\")\n",
    "\n",
    "    # Load and normalize features\n",
    "    labels = np.load(f\"./data/npz/{suffix}/data_features.npz\")\n",
    "    features = sklearn.preprocessing.StandardScaler().fit_transform(features)\n",
    "\n",
    "    # Load edge indices\n",
    "    edge_index = np.load(f\"./data/npz/{suffix}/edge_index.npz\")\n",
    "\n",
    "    # Print dataset statistics\n",
    "    print_statistics(features, labels, edge_index, train_mask, val_mask, test_mask)\n",
    "\n",
    "    if not balanced:\n",
    "        all_labels = [0, 1, 2, 3, 4, 5]\n",
    "        chosen_labels = [0, 1, 3, 5]\n",
    "\n",
    "        print(\"[Before unbalancing] Class distribution in the training set:\")\n",
    "        for label in all_labels:\n",
    "                count = np.sum(labels[train_mask] == label)\n",
    "                print(f\"Label {label}: {count} samples\")\n",
    "        print(\"[Before unbalancing] Class distribution in the validation set:\")\n",
    "        for label in all_labels:\n",
    "                count = np.sum(labels[val_mask] == label)\n",
    "                print(f\"Label {label}: {count} samples\")\n",
    "\n",
    "        print(\"[Before unbalancing] Class distribution in the test set:\")\n",
    "        for label in all_labels:\n",
    "                count = np.sum(labels[test_mask] == label)\n",
    "                print(f\"Label {label}: {count} samples\")\n",
    "\n",
    "        chosen_indices = np.where(np.isin(labels[train_mask], chosen_labels))[0]\n",
    "        train_indices, test_indices = train_test_split(chosen_indices, test_size=0.8, stratify=labels[train_mask][chosen_indices])\n",
    "                \n",
    "        new_train_mask = torch.full_like(train_mask, False)\n",
    "        new_train_mask[train_indices] = True\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "                if label in chosen_labels and new_train_mask[i] == False:\n",
    "                        train_mask[i] = False\n",
    "\n",
    "        train_mask[train_indices] = True\n",
    "        test_mask[test_indices] = True\n",
    "\n",
    "        print(\"Class distribution in the training set:\")\n",
    "        for label in all_labels:\n",
    "                count = np.sum(labels[train_mask] == label)\n",
    "                print(f\"Label {label}: {count} samples\")\n",
    "\n",
    "        print(\"Class distribution in the validation set:\")\n",
    "        for label in all_labels:\n",
    "                count = np.sum(labels[val_mask] == label)\n",
    "                print(f\"Label {label}: {count} samples\")\n",
    "\n",
    "        print(\"Class distribution in the test set:\")\n",
    "        for label in all_labels:\n",
    "                count = np.sum(labels[test_mask] == label)\n",
    "                print(f\"Label {label}: {count} samples\")\n",
    "\n",
    "        return features, labels, edge_index, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b726dd58-d4a0-427b-99ee-766ebf2c5c62",
   "metadata": {},
   "source": [
    "## Graphs and Graph Constructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b907c-4c74-425d-9737-3c168fbb78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(x, y, edge_index, train_mask, val_mask, test_mask):\n",
    "    \"\"\"\n",
    "    Construct a NetworkX graph from node features, labels, and edge information.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray or torch.Tensor): Node features with shape (num_nodes, feature_dim).\n",
    "        y (np.ndarray or torch.Tensor): Node labels with shape (num_nodes,).\n",
    "        edge_index (torch.Tensor or list of tuples): Edge indices either in PyTorch Geometric format (2xN tensor) or standard edge list format.\n",
    "        train_mask (np.ndarray or list): Boolean mask indicating training nodes.\n",
    "        val_mask (np.ndarray or list): Boolean mask indicating validation nodes.\n",
    "        test_mask (np.ndarray or list): Boolean mask indicating test nodes.\n",
    "\n",
    "    Returns:\n",
    "        nx.Graph: A NetworkX graph with nodes having attributes for features, labels, and masks, and edges with default weights.\n",
    "    \"\"\"\n",
    "    # Construct NetworkX Graph\n",
    "    nodes = [i for i in range(x.shape[0])]\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes with attributes\n",
    "    for i in nodes:\n",
    "        G.add_node(i, x=x[i], y=y[i], train=train_mask[i], \n",
    "                   val=val_mask[i], test=test_mask[i])\n",
    "    \n",
    "    # Handle edge_index input (PyTorch Geometric format)\n",
    "    if isinstance(edge_index, torch.Tensor) and edge_index.dim() == 2 and edge_index.shape[0] == 2:\n",
    "        edge_list = edge_index.t().tolist()\n",
    "    else:\n",
    "        # Assuming edge_index is in standard edge list format\n",
    "        edge_list = edge_index\n",
    "\n",
    "    # Add edges with a default weight of 1\n",
    "    weighted_edges = [(edge[0], edge[1], 1) for edge in edge_list]\n",
    "    G.add_weighted_edges_from(weighted_edges)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942c1b2-68ca-4a4e-8dc8-85da2d508cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_graph(G, multilabel = True):\n",
    "    \"\"\"\n",
    "    Split the graph into training, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        G (nx.Graph): The input NetworkX graph with node attributes specifying train, val, and test masks.\n",
    "        multilabel (bool): Flag indicating if the graph is multilabel. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - x_train (np.ndarray or torch.Tensor): Node features for training nodes.\n",
    "            - y_train (np.ndarray or torch.Tensor): Node labels for training nodes.\n",
    "            - edge_train (np.ndarray or torch.Tensor): Edge indices for training nodes.\n",
    "            - train_mask (np.ndarray or torch.Tensor): Boolean mask for training nodes.\n",
    "            - x_val (np.ndarray or torch.Tensor): Node features for validation nodes.\n",
    "            - y_val (np.ndarray or torch.Tensor): Node labels for validation nodes.\n",
    "            - edge_val (np.ndarray or torch.Tensor): Edge indices for validation nodes.\n",
    "            - val_mask (np.ndarray or torch.Tensor): Boolean mask for validation nodes.\n",
    "            - x_test (np.ndarray or torch.Tensor): Node features for test nodes.\n",
    "            - y_test (np.ndarray or torch.Tensor): Node labels for test nodes.\n",
    "            - edge_test (np.ndarray or torch.Tensor): Edge indices for test nodes.\n",
    "            - test_mask (np.ndarray or torch.Tensor): Boolean mask for test nodes.\n",
    "    \"\"\"\n",
    "    print(\"Splitting Graph...\")\n",
    "    print(\"=============== Graph Splitting ===============\")\n",
    "    \n",
    "    # Get complete test graph\n",
    "    x_test, y_test, edge_test, _, _, test_mask = convert_graph_to_tensor(G, multilabel=multilabel)\n",
    "    \n",
    "    print(f\"Unlabeled + Test + Validation + Training graph nodes: {x_test.shape[0]}\")\n",
    "    print(f\"Unlabeled + Test + Validation + Training graph edges: {edge_test.shape[0]}\")\n",
    "    print(f\"Total test nodes: {test_mask.sum()}\")\n",
    "    \n",
    "    # Get training + val graph\n",
    "    # remove all test nodes\n",
    "    test_nodes = []\n",
    "    for node in G.nodes(data=True):\n",
    "        if node[1]['test']:\n",
    "            test_nodes.append(node[0])\n",
    "    G.remove_nodes_from(test_nodes)\n",
    "    G = nx.convert_node_labels_to_integers(G, first_label=0, ordering='default')\n",
    "    x_val, y_val, edge_val, _, val_mask, _ = convert_graph_to_tensor(G, multilabel=multilabel)\n",
    "    \n",
    "    print(f\"Unlabeled + Validation + Training graph nodes: {x_val.shape[0]}\")\n",
    "    print(f\"Unlabeled + Validation + Training graph edges: {edge_val.shape[0]}\")\n",
    "    print(f\"Total val nodes: {val_mask.sum()}\")\n",
    "    # Get training graph\n",
    "    # remove all val nodes\n",
    "    val_nodes = []\n",
    "    for node in G.nodes(data=True):\n",
    "        if node[1]['val']:\n",
    "            val_nodes.append(node[0])\n",
    "    G.remove_nodes_from(val_nodes)\n",
    "    G = nx.convert_node_labels_to_integers(G, first_label=0, ordering='default')\n",
    "    \n",
    "    x_train, y_train, edge_train, train_mask, _, _ = convert_graph_to_tensor(G, multilabel = multilabel)\n",
    "    \n",
    "    print(f\"Unlabeled + Training graph nodes: {x_train.shape[0]}\")\n",
    "    print(f\"Unlabeled + Training graph edges: {edge_train.shape[0]}\")\n",
    "    print(f\"Total train nodes: {train_mask.sum()}\")\n",
    "    print()\n",
    "    \n",
    "    return (x_train, y_train, edge_train, train_mask, x_val, y_val, edge_val, \n",
    "            val_mask, x_test, y_test, edge_test, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a1c21-f0bc-46a1-b5c8-6ccd2777c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_graph_to_tensor(G, multilabel = True):\n",
    "    \"\"\"\n",
    "    Convert a NetworkX graph into tensors or numpy arrays for use in machine learning models.\n",
    "\n",
    "    Args:\n",
    "        G (nx.Graph): The input NetworkX graph with node attributes for features, labels, and masks.\n",
    "        multilabel (bool): Flag indicating if the graph is multilabel. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - x (np.ndarray or torch.Tensor): Node features.\n",
    "            - y (np.ndarray or torch.Tensor): Node labels.\n",
    "            - edge_index (np.ndarray or torch.Tensor): Edge indices.\n",
    "            - train_mask (np.ndarray or torch.Tensor): Boolean mask for training nodes.\n",
    "            - val_mask (np.ndarray or torch.Tensor): Boolean mask for validation nodes.\n",
    "            - test_mask (np.ndarray or torch.Tensor): Boolean mask for test nodes.\n",
    "    \"\"\"\n",
    "    x = np.empty((G.number_of_nodes(),G.nodes[0]['x'].shape[0]))\n",
    "    if multilabel:\n",
    "        y = np.empty((G.number_of_nodes(),G.nodes[0]['y'].shape[0]),dtype = 'int')\n",
    "    else:\n",
    "        y = np.empty((G.number_of_nodes(),),dtype = 'int')\n",
    "        \n",
    "    edge_index = np.array([edge for edge in G.edges()])\n",
    "    train_mask = np.empty((G.number_of_nodes(),),dtype = 'bool')\n",
    "    val_mask = np.empty((G.number_of_nodes(),),dtype = 'bool')\n",
    "    test_mask = np.empty((G.number_of_nodes(),),dtype = 'bool')\n",
    "    \n",
    "    for node in G.nodes(data=True):\n",
    "        x[node[0],:] = node[1]['x']\n",
    "        if multilabel:\n",
    "            y[node[0],:] = node[1]['y']\n",
    "        else:\n",
    "            y[node[0]] = node[1]['y']\n",
    "        \n",
    "        train_mask[node[0]] = node[1]['train']\n",
    "        val_mask[node[0]] = node[1]['val']\n",
    "        test_mask[node[0]] = node[1]['test']\n",
    "    \n",
    "    return x, y, edge_index, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab18f6-8cd8-44db-a653-94a441de7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_normalized_adj(edge_index, num_nodes):\n",
    "    \"\"\"\n",
    "    Construct a normalized adjacency matrix from edge indices.\n",
    "\n",
    "    Args:\n",
    "        edge_index (np.ndarray or torch.Tensor): Edge indices in the format [2, num_edges].\n",
    "        num_nodes (int): Number of nodes in the graph.\n",
    "\n",
    "    Returns:\n",
    "        SparseTensor: Normalized adjacency matrix with self-loops added and GCN normalization applied.\n",
    "    \"\"\"\n",
    "    edge_index = torch.tensor(edge_index)\n",
    "    edge_index = torch.transpose(edge_index,0,1)\n",
    "    edge_index_flip = torch.flip(edge_index,[0]) # re-adds flipped edges that were removed by networkx\n",
    "    edge_index = torch.cat((edge_index, edge_index_flip), 1)\n",
    "    adj = SparseTensor(row=edge_index[0], col=edge_index[1], sparse_sizes=(num_nodes,num_nodes))\n",
    "    adj = adj.set_diag() # adding self loops\n",
    "    adj = gcn_norm(adj, add_self_loops=False) # normalization\n",
    "\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6247bc6-c5b9-45b6-a78a-8badef0c04ac",
   "metadata": {},
   "source": [
    "## Get Metrics of the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733accae-0c75-4a77-86b1-b1d494744d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_to_label(out):\n",
    "    \"\"\"\n",
    "    Convert logits to predicted labels using argmax\n",
    "\n",
    "    Args:\n",
    "        out (torch.Tensor): Logits tensor\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Predicated labels\n",
    "    \"\"\"\n",
    "    return out.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e8356-cd24-4b98-b017-792f501de818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(logits, y):\n",
    "    if y.dim() == 1: # Multi-class\n",
    "        y_pred = logit_to_label(logits)\n",
    "        cm = confusion_matrix(y.cup(), y_pred.cpu())\n",
    "        FP = cm.sum(axis=0) - np.diag(cm)\n",
    "        FN = cm.sum(axis=1) - np.diag(cm)\n",
    "        TP = np.diag(cm)\n",
    "        TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "        acc = np.diag(cm).sum() / cm.sum()\n",
    "        micro_f1 = acc # accuracy for multi-class\n",
    "        sens = TP.sum() / (TP.sum() + FN.sum())\n",
    "        spec = TN.sum() / (TN.sum() + FP.sum())\n",
    "    else: # Multi-label\n",
    "        y_pred = logits >= 0\n",
    "        y_true = y >= 0.5\n",
    "\n",
    "        tp = int((y_true & y_pred).sum())\n",
    "        tn = int((~y_true & ~y_pred).sum())\n",
    "        fp = int((~y_true & y_pred).sum())\n",
    "        fn = int((y_true & ~y_pred).sum())\n",
    "\n",
    "        acc = (tp + tn)/(tp + fp + tn + fn)\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        micro_f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        sens = (tp)/(tp + fn)\n",
    "        spec = (tn)/(tn + fp)\n",
    "\n",
    "    return acc, micro_f1, sens, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae01a0-14af-4a81-a311-a6da163c6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_degree_centrality(graph):\n",
    "    edge_degree = {}\n",
    "\n",
    "    # Iterate over edges\n",
    "    for edge in graph.edges():\n",
    "        u, v = edge\n",
    "\n",
    "        # Compute degrees of the nodes incident to the edges\n",
    "        degree_u = graph.degree(u)\n",
    "        degree_v = graph.degree(v)\n",
    "\n",
    "        # Calculate average degree\n",
    "        avg_degree = (degree_u + degree_v) / 2\n",
    "\n",
    "        # Assign average degrees as edge degree centrality\n",
    "        edge_degree[edge] = avg_degree\n",
    "    return edge_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b85128e-c061-4ed6-8d1a-73d26a7c5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_metric_compute(metric, graph):\n",
    "    edges = {}\n",
    "\n",
    "    # Iterate over edges\n",
    "    for edge in graph.edges():\n",
    "        u, v = edge\n",
    "\n",
    "        metric_u = metric[u]\n",
    "        metric_v = metric[v]\n",
    "\n",
    "        avg_centrality = (metric_u + metric_v) / 2\n",
    "\n",
    "        edges[edge] = avg_centrality\n",
    "\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcca2e1-3da6-4bf8-8907-7047aa03f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_homophily_ratios(adj, x, y):\n",
    "    homophily_ratios = []\n",
    "\n",
    "    # Extract COO format components\n",
    "    row, col, _ = adj.coo()\n",
    "\n",
    "    for r,c in zip(row.tolist(), col.tolist()):\n",
    "        # Extract features and labels using row and col indices\n",
    "        x1, y1 = x[r], y[r]\n",
    "        x2, y2 = x[c], y[c]\n",
    "\n",
    "        # Calculate similarity in features (assuming features are numpy arrays)\n",
    "        feature_similarity = np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))\n",
    "\n",
    "        # Calculate similarity in labels\n",
    "        label_similarity = 1 if y1 = y2 else 0\n",
    "\n",
    "        # Homophily ratio can be a combination of both similarities\n",
    "        homophily_ratio = 0.5 * feature_similarity + 0.5 * label_similarity\n",
    "\n",
    "        homophily_ratios.append(homophily_ratio)\n",
    "\n",
    "    return homophily_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d0b22-aafd-4819-86cf-a495e10f2e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
